# Project Analysis Prompt

Analyze the following project structure and file contents.

## From: Dockerfile

---
File: Dockerfile
---
```
FROM python:3.10-slim
RUN apt-get update && apt-get install -y supervisor && \
    mkdir -p /var/log/supervisor
WORKDIR /src

COPY . /src

RUN pip install --no-cache-dir -r requirements.txt

COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

# M·ªü c·ªïng cho FastAPI
EXPOSE 8000

# üî• Ch·∫°y supervisor (thay v√¨ uvicorn)
CMD ["/usr/bin/supervisord"]
```

## From: app

---
File: __pycache__/api.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x8e in position 10: invalid start byte

---
File: __pycache__/main.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xc0 in position 8: invalid start byte

---
File: __pycache__/run_worker.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x80 in position 8: invalid start byte

---
File: __pycache__/worker_settings.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x8a in position 10: invalid start byte

---
File: db/__pycache__/client.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x8e in position 10: invalid start byte

---
File: db/__pycache__/models.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xbe in position 8: invalid start byte

---
File: db/client.py
---
```python
# db/client.py
import os
from dotenv import load_dotenv
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.db.models import Base

load_dotenv()
DATABASE_URL = os.getenv("DATABASE_URL")

# Create engine and sessionmaker
engine = create_engine(DATABASE_URL, pool_pre_ping=True)
SessionLocal = sessionmaker(bind=engine, autoflush=False, autocommit=False)

def init_db():
    Base.metadata.create_all(bind=engine)

# trong app/db/client.py
from contextlib import contextmanager


@contextmanager
def get_session():
    session = SessionLocal()
    try:
        yield session
        session.commit()
    except Exception:
        session.rollback()
        raise
    finally:
        session.close()
```

---
File: db/models.py
---
```python
# db/models.py
from sqlalchemy import (
    Column, String, Integer, DateTime, Enum as PgEnum,
    ForeignKey, Text
)
from sqlalchemy.dialects.postgresql import ARRAY
from sqlalchemy.orm import declarative_base, relationship
from datetime import datetime, timezone
from enum import Enum
import uuid

Base = declarative_base()

# Enums
class TaskType(str, Enum):
    parseDocumentImages = "parseDocumentPDF"
    documentParsing = "documentParsing"

class TaskStatus(str, Enum):
    pending = "pending"
    done = "done"
    failed = "failed"
    cancelled = "cancelled"

# Models
class Task(Base):
    __tablename__ = "task"

    id = Column(String, primary_key=True, default=lambda: uuid.uuid4().hex)
    type = Column(PgEnum(TaskType), nullable=False)
    status = Column(PgEnum(TaskStatus), default=TaskStatus.pending, nullable=False)
    result = Column(Text, nullable=True)
    error = Column(Text, nullable=True)
    createdAt = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
    updatedAt = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))

    inference_logs = relationship("InferenceLog", back_populates="task", cascade="all, delete-orphan")

class InferenceLog(Base):
    __tablename__ = "inferencelog"

    id = Column(String, primary_key=True, default=lambda: uuid.uuid4().hex)
    imageUrls = Column(Text, nullable=True)
    rawOutput = Column(Text, nullable=False)
    objectKeys = Column(ARRAY(String), nullable=True)
    objectUrls = Column(ARRAY(String), nullable=True)
    num_input_token = Column(Integer, nullable=False)
    num_output_token = Column(Integer, nullable=False)
    page_order = Column(Integer, nullable=True)
    createdAt = Column(DateTime(timezone=True), default=lambda: datetime.now(timezone.utc))
    requestId = Column(String, ForeignKey("task.id"), nullable=True)
    task = relationship("Task", back_populates="inference_logs")
```

---
File: lib/__pycache__/redis_client.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x93 in position 8: invalid start byte

---
File: lib/__pycache__/worker_core.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x8e in position 8: invalid start byte

---
File: lib/redis_client.py
---
```python
import os
import redis.asyncio as redis

redis_host = os.getenv("REDIS_HOST", "localhost")
redis_port = int(os.getenv("REDIS_PORT", 6379))

r = redis.Redis(host=redis_host, port=redis_port)
```

---
File: lib/worker_core.py
---
```python
import asyncio
import json
from typing import Callable, Awaitable, Dict, Optional
from redis.asyncio import Redis
import os
redis_host = os.getenv("REDIS_HOST", "localhost")
redis_port = int(os.getenv("REDIS_PORT", 6379))
class SimpleRedisWorker:
    def __init__(self, queue_name: str = "tasks"):
        self.redis = Redis(host=redis_host, port=redis_port, decode_responses=True)
        self.queue_name = queue_name
        self.tasks: Dict[str, Callable[..., Awaitable]] = {}
        self.semaphores: Dict[str, asyncio.Semaphore] = {}
        self.retries: Dict[str, int] = {}
        self.running = False

    def task(
        self,
        name: Optional[str] = None,
        max_concurrency: Optional[int] = None,
        max_retries: int = 0
    ):
        """Decorator ƒë·ªÉ ƒëƒÉng k√Ω task v·ªõi optional concurrency limit v√† retry."""
        def decorator(func: Callable[..., Awaitable]):
            task_name = name or func.__name__
            self.tasks[task_name] = func
            if max_concurrency:
                self.semaphores[task_name] = asyncio.Semaphore(max_concurrency)
            if max_retries > 0:
                self.retries[task_name] = max_retries
            return func
        return decorator

    async def enqueue(self, task_name: str, *args, **kwargs):
        data = {"task": task_name, "args": args, "kwargs": kwargs}
        await self.redis.rpush(self.queue_name, json.dumps(data))

    async def run_worker(self):
        print(f"[Worker] Listening on queue '{self.queue_name}'...")
        self.running = True
        while self.running:
            item = await self.redis.blpop(self.queue_name, timeout=1)
            if not item:
                continue

            _, raw = item
            job = json.loads(raw)
            task_name = job.get("task")
            func = self.tasks.get(task_name)

            if not func:
                print(f"[Worker] Unknown task: {task_name}")
                continue

            args = job.get("args", [])
            kwargs = job.get("kwargs", {})
            sem = self.semaphores.get(task_name)
            max_retries = self.retries.get(task_name, 0)

            if sem:
                asyncio.create_task(
                    self._execute_with_limit_and_retry(sem, func, args, kwargs, max_retries)
                )
            else:
                asyncio.create_task(
                    self._execute_with_retry(func, args, kwargs, max_retries)
                )

    async def _execute_with_limit_and_retry(
        self,
        sem: asyncio.Semaphore,
        func: Callable[..., Awaitable],
        args: list,
        kwargs: dict,
        max_retries: int
    ):
        async with sem:
            await self._execute_with_retry(func, args, kwargs, max_retries)

    async def _execute_with_retry(
        self,
        func: Callable[..., Awaitable],
        args: list,
        kwargs: dict,
        max_retries: int
    ):
        attempt = 0
        while True:
            try:
                await func(*args, **kwargs)
                return
            except Exception as e:
                attempt += 1
                if attempt <= max_retries:
                    print(f"[Worker] Task '{func.__name__}' failed (attempt {attempt}/{max_retries}), retrying...")
                    await asyncio.sleep(1)  # backoff
                    continue
                else:
                    print(f"[Worker] Task '{func.__name__}' failed after {max_retries} retries: {e}")
                    return

    async def stop(self):
        self.running = False
```

---
File: main.py
---
```python
from fastapi import FastAPI, UploadFile, File, HTTPException, Depends, Header
from fastapi.responses import FileResponse, HTMLResponse
from pathlib import Path
from contextlib import asynccontextmanager

from blake3 import blake3
import aiofiles
import os
import httpx
import asyncio

from pydantic import BaseModel
from app.worker.instance import worker

from app.db.client import init_db, get_session
from app.services.create_task import create_task, get_task_by_id
from dotenv import load_dotenv
from app.db.models import Task, TaskType
from app.postprocessing.convert2html import convert_text2html
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from typing import Optional
import aiohttp
import os
from app.utils.agent import init_api_key_zset# --- C·∫•u h√¨nh th∆∞ m·ª•c ---
PRIVATE_DIR = Path("media").resolve()
PRIVATE_DIR.mkdir(exist_ok=True)
keys = os.environ.get("GEMINI_API_KEY", "").split(",")

# --- Init DB khi start server ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    init_db()
    await init_api_key_zset(keys)
    yield

app = FastAPI(lifespan=lifespan)
# --- Middleware x√°c th·ª±c (gi·∫£ l·∫≠p) ---
def verify_access(x_token: str = Header(None)):
    if x_token != os.environ.get("X_FILE_TOKEN"):
        raise HTTPException(status_code=401, detail="Unauthorized")


# --- Serve file PRIVATE (PDF, ·∫£nh g·ªëc) ---
@app.get("/media/{file_path:path}")
def serve_private_file(file_path: str, _: str = Depends(verify_access)):
    full_path = (PRIVATE_DIR / file_path).resolve()

    if not str(full_path).startswith(str(PRIVATE_DIR)):
        raise HTTPException(status_code=403, detail="Access denied")

    if not full_path.exists() or not full_path.is_file():
        raise HTTPException(status_code=404, detail="File not found")

    return FileResponse(full_path, filename=full_path.name)



async def save_file(file_bytes: bytes, file_type: str) -> Path:
    # T√≠nh hash
    file_hash = blake3(file_bytes).hexdigest()

    # T·∫°o th∆∞ m·ª•c con
    subdir = PRIVATE_DIR / file_hash[:2] / file_hash[2:4]
    subdir.mkdir(parents=True, exist_ok=True)

    filename = f"{file_hash}.{file_type}"
    filepath = subdir / filename

    # Ch·ªâ ghi file n·∫øu n√≥ ch∆∞a t·ªìn t·∫°i
    if not filepath.exists():
        temp_path = subdir / f".{filename}.tmp"
        # Ghi v√†o file t·∫°m
        async with aiofiles.open(temp_path, "wb") as f:
            await f.write(file_bytes)
        # Rename file t·∫°m th√†nh file th·∫≠t (atomic)
        try:
            temp_path.rename(filepath)
        except Exception as e:
            # C√≥ th·ªÉ file ƒë√£ ƒë∆∞·ª£c t·∫°o b·ªüi m·ªôt process kh√°c trong l√∫c ghi, kh√¥ng sao c·∫£
            print(f"Could not rename temp file, possibly due to race condition: {e}")
            if temp_path.exists():
                temp_path.unlink() # X√≥a file t·∫°m ƒëi

    # Lu√¥n t√≠nh to√°n v√† tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi
    relative_path = filepath.relative_to(PRIVATE_DIR)
    return relative_path




@app.get("/logs")
def read_latest_worker_log():
    # üìÅ Tr·ªè t·ªõi th∆∞ m·ª•c ch·ª©a log
    log_dir = (Path(__file__).resolve().parent.parent / "logs").resolve()

    if not log_dir.exists() or not log_dir.is_dir():
        raise HTTPException(status_code=404, detail="Log directory not found")

    # üîç T√¨m t·∫•t c·∫£ c√°c file b·∫Øt ƒë·∫ßu b·∫±ng "workers_log"
    log_files = list(log_dir.glob("workers_log*.txt*"))

    if not log_files:
        raise HTTPException(status_code=404, detail="No log files found")

    # üïì Ch·ªçn file m·ªõi nh·∫•t d·ª±a v√†o th·ªùi gian s·ª≠a ƒë·ªïi
    latest_log = max(log_files, key=lambda f: f.stat().st_mtime)

    return FileResponse(
        path=latest_log,
        media_type="text/plain",
        filename=latest_log.name,
        headers={"Content-Disposition": f"inline; filename={latest_log.name}"}
    )
async def get_file_bytes(url):
    headers = {"x-token": os.environ.get("X_FILE_TOKEN")}
    async with httpx.AsyncClient() as client:
        resp = await client.get(url, headers=headers)
        return resp.content
# --- API Upload PDF ---


@app.post("/process-pdf")
async def handle_pdf(
    file: Optional[UploadFile] = File(None),
    url: Optional[str] = Form(None)  # D√πng Form ƒë·ªÉ nh·∫≠n URL t·ª´ body
):
    file_bytes = None
    filename = None

    if file:
        if not file.filename.lower().endswith(".pdf"):
            raise HTTPException(status_code=400, detail="Ch·ªâ ch·∫•p nh·∫≠n file PDF.")
        file_bytes = await file.read()
        filename = file.filename
        await file.close()
    elif url:
        if not url.lower().endswith(".pdf"):
            raise HTTPException(status_code=400, detail="URL ph·∫£i tr·ªè ƒë·∫øn m·ªôt file PDF.")
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as resp:
                    if resp.status != 200:
                        raise HTTPException(status_code=400, detail="Kh√¥ng th·ªÉ t·∫£i file t·ª´ URL.")
                    file_bytes = await resp.read()
                    filename = os.path.basename(url)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"L·ªói khi t·∫£i file: {str(e)}")
    else:
        raise HTTPException(status_code=400, detail="C·∫ßn cung c·∫•p file ho·∫∑c URL.")

    # L∆∞u file
    relative_file_path_str = await save_file(file_bytes=file_bytes, file_type='pdf')
    file_url = f"http://localhost:8000/media/{relative_file_path_str}"

    # T·∫°o task
    task_id = None
    with get_session() as session:
        task = create_task(task_type=TaskType("parseDocumentPDF"), session=session)
        task_id = task.id

    await worker.enqueue("process_pdf", task_id, file_url)

    return {"task_id": task_id}


class Text2Slurp(BaseModel):
    text: str

@app.post("/document-parsing")
async def documentParsing(body: Text2Slurp):
    task_id=None
    with get_session() as session:
        task = create_task(task_type=TaskType("documentParsing"), session=session)
        task_id = task.id
    await worker.enqueue("documentParsing", task_id, body.text)
    return {"task_id": task_id}


# Route: GET one task theo id
@app.get("/tasks/{task_id}")
def get_task(task_id: str):
    with get_session() as session:
        task = get_task_by_id(task_id=task_id, session=session)
        if task:
                return {
                    "id": str(task.id),
                    "type": task.type.value,
                    "status": task.status.value,
                    "result": task.result,
                    "createdAt": task.createdAt.isoformat(),
                    "updatedAt": task.updatedAt.isoformat(),
                }
        raise HTTPException(status_code=404, detail="Task not found")

class Text2Html(BaseModel):
    text: str

@app.post("/text2html/")
async def text2html(body: Text2Html):
    
    return HTMLResponse(content=convert_text2html(body.text) , media_type="text/html")

# # --- Serve file PUBLIC (figure) ---
# @app.get("/media-public/{file_path:path}")
# def serve_public_file(file_path: str):
#     full_path = (PUBLIC_DIR / file_path).resolve()

#     if not str(full_path).startswith(str(PUBLIC_DIR)):
#         raise HTTPException(status_code=403, detail="Access denied")

#     if not full_path.exists() or not full_path.is_file():
#         raise HTTPException(status_code=404, detail="File not found")

#     return FileResponse(full_path, filename=full_path.name)
```

---
File: ml_models/__init__.py
---
```python

```

---
File: ml_models/__pycache__/__init__.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xc9 in position 8: invalid continuation byte

---
File: ml_models/__pycache__/image_classifier.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x9c in position 8: invalid start byte

---
File: ml_models/__pycache__/rfdetr.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x9c in position 8: invalid start byte

---
File: ml_models/image_classifier.py
---
```python
import onnxruntime
import numpy as np
import cv2
import os

# --- Config ---
IMAGE_SIZE = 512
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ONNX_MODEL_PATH = os.path.join(BASE_DIR, 'weights', 'ghostnet_classifier.onnx')
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]
THRESHOLD_POSITIVE = 0.65


# --- Image Classifier Class ---
class ImageClassifier:
    def __init__(self, model_path: str):
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ONNX model not found at {model_path}")
        self.session = onnxruntime.InferenceSession(model_path)
        print(f"Model loaded: {model_path}")

    @staticmethod
    def softmax(x):
        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))
        return e_x / e_x.sum(axis=1, keepdims=True)

    def preprocess(self, np_img):
        img = cv2.resize(np_img, (IMAGE_SIZE, IMAGE_SIZE)).astype(np.float32) / 255.0
        img = (img - IMAGENET_MEAN) / IMAGENET_STD
        img = img.astype(np.float32)
        img = img.transpose(2, 0, 1)  # HWC -> CHW
        return np.expand_dims(img, axis=0)  # (1, 3, H, W)

    def predict(self, np_img: np.ndarray):
        try:
            input_tensor = self.preprocess(np_img)
            ort_inputs = {self.session.get_inputs()[0].name: input_tensor}
            ort_outputs = self.session.run(None, ort_inputs)
            logits = ort_outputs[0]
            probs = self.softmax(logits)
            confidence = probs[0][1]
            predicted_class_index = 1 if confidence > THRESHOLD_POSITIVE else 0
            return predicted_class_index, logits, confidence
        except Exception as e:
            print(f"L·ªói khi x·ª≠ l√Ω ·∫£nh numpy: {e}")
            return None, None, None

classifier = ImageClassifier(ONNX_MODEL_PATH)

# # --- Test ---
# if __name__ == "__main__":
#     image_path = "your_image.png"  # ‚Üê Thay b·∫±ng ·∫£nh c·ªßa b·∫°n
#     if not os.path.exists(image_path):
#         print(f"·∫¢nh kh√¥ng t·ªìn t·∫°i: {image_path}")
#         exit(1)

#     np_image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)
#     classifier = ImageClassifier(ONNX_MODEL_PATH)

#     class_to_idx = {0: "negative", 1: "positive"}
#     pred_idx, logits, confidence = classifier.predict(np_image)

#     if pred_idx is not None:
#         print(f"Prediction: {class_to_idx[pred_idx]} (Confidence: {confidence:.2%})")
#     else:
#         print("Kh√¥ng th·ªÉ d·ª± ƒëo√°n.")
```

---
File: ml_models/rfdetr.py
---
```python
# This file contains code licensed under the Apache License, Version 2.0.
# See NOTICE for more details.

import io
import requests
import onnxruntime as ort
import numpy as np
import os
from PIL import Image, ImageDraw, ImageFont


def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def box_cxcywh_to_xyxy_numpy(x):
    x_c, y_c, w, h = np.split(x, 4, axis=-1)
    b = np.concatenate([
        x_c - 0.5 * np.clip(w, a_min=0.0, a_max=None),
        y_c - 0.5 * np.clip(h, a_min=0.0, a_max=None),
        x_c + 0.5 * np.clip(w, a_min=0.0, a_max=None),
        y_c + 0.5 * np.clip(h, a_min=0.0, a_max=None)
    ], axis=-1)
    return b

class RTDETR_ONNX:
    MEANS = [0.485, 0.456, 0.406]
    STDS = [0.229, 0.224, 0.225]

    def __init__(self, onnx_model_path):

        # Load the ONNX model and initialize the ONNX Runtime session
        self.ort_session = ort.InferenceSession(onnx_model_path)
        print(f'Model RFDETR loaded: {onnx_model_path}')
        # Get input shape
        input_info = self.ort_session.get_inputs()[0]
        self.input_height, self.input_width = input_info.shape[2:]


    def _preprocess_image(self, image):
        """Preprocess the input image for inference."""

        # Resize the image to the model's input size
        image = image.resize((self.input_width, self.input_height))

        # Convert image to numpy array and normalize pixel values
        image = np.array(image).astype(np.float32) / 255.0

        # Normalize
        image = ((image - self.MEANS) / self.STDS).astype(np.float32)

        # Change dimensions from HWC to CHW
        image = np.transpose(image, (2, 0, 1))

        # Add batch dimension
        image = np.expand_dims(image, axis=0)

        return image

    def _post_process(self, outputs, origin_height, origin_width, confidence_threshold, max_number_boxes):
        """Post-process the model's output to extract bounding boxes and class information."""
        # Get the bounding box and class scores
        pred_boxes, pred_logits = outputs

        # Apply sigmoid activation
        prob = sigmoid(pred_logits)

        # Get the top-k values and indices
        flat_prob = prob[0].flatten()
        topk_indexes = np.argsort(flat_prob)[-max_number_boxes:][::-1]
        topk_values = np.take_along_axis(flat_prob, topk_indexes, axis=0)
        scores = topk_values
        topk_boxes = topk_indexes // pred_logits.shape[2]
        labels = topk_indexes % pred_logits.shape[2]

        # Gather boxes corresponding to top-k indices
        boxes = box_cxcywh_to_xyxy_numpy(pred_boxes[0])
        boxes = np.take_along_axis(boxes, np.expand_dims(topk_boxes, axis=-1).repeat(4, axis=-1), axis=0)

        # Rescale box locations
        target_sizes = np.array([[origin_height, origin_width]])
        img_h, img_w = target_sizes[:, 0], target_sizes[:, 1]
        scale_fct = np.stack([img_w, img_h, img_w, img_h], axis=1)
        boxes = boxes * scale_fct[0, :]

        # Filter detections based on the confidence threshold
        high_confidence_indices = np.argmin(scores > confidence_threshold)
        scores = scores[:high_confidence_indices]
        labels = labels[:high_confidence_indices]
        boxes = boxes[:high_confidence_indices]

        return scores, labels, boxes

    def run_inference(self, image, confidence_threshold=0.5, max_number_boxes=100):
        """Run the model inference and return the raw outputs."""

        # Load the image
        image = image.convert('RGB')
        origin_width, origin_height = image.size

        # Preprocess the image
        input_image = self._preprocess_image(image)

        # Get input name from the model
        input_name = self.ort_session.get_inputs()[0].name

        # Run the model
        outputs = self.ort_session.run(None, {input_name: input_image})

        # Post-process
        return self._post_process(outputs, origin_height, origin_width, confidence_threshold, max_number_boxes)


    def save_detections(self, image, boxes, labels, save_image_path):
        """Draw bounding boxes and class labels on the original image."""
        # Load the original image
        image = image.convert('RGB')

        draw = ImageDraw.Draw(image)

        # Loop over the boxes
        for i, box in enumerate(boxes.astype(int)):

            # Draw the rectangle (box) on the image
            draw.rectangle(box.tolist(), outline="green", width=4)

            # Using default font
            font = ImageFont.load_default()

            # Position the text inside the rectangle
            text_x = box[0] + 10  # Left margin for text
            text_y = box[1] + 10  # Top margin for text
            draw.text((text_x, text_y), str(labels[i]), fill="red", font=font)

        # Save the image with the rectangle and text
        image.save(save_image_path)
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MODEL_PATH = os.path.join(BASE_DIR, 'weights', 'rfdetr-figure.onnx')
rtdetr_model = RTDETR_ONNX(MODEL_PATH)
```

---
File: postprocessing/__pycache__/convert2html.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xbb in position 9: invalid start byte

---
File: postprocessing/__pycache__/raw_response.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x8d in position 10: invalid start byte

---
File: postprocessing/__pycache__/replaceimgfig.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xf7 in position 8: invalid start byte

---
File: postprocessing/__pycache__/slurp2json.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xae in position 8: invalid start byte

---
File: postprocessing/convert2html.py
---
```python
import re
from latex2mathml.converter import convert


def latex_to_inline_mathml(text: str) -> str:
    pattern = re.compile(r"""
        \\\[(.+?)\\\]      |   # group 2: \[...\]
        \\\((.+?)\\\)      |   # group 3: \(...\)
    """, re.VERBOSE | re.DOTALL)

    def repl(m):
        groups = [g for g in m.groups() if g is not None]
        if not groups:
            # Kh√¥ng c√≥ g√¨ match, tr·∫£ nguy√™n ƒëo·∫°n
            return m.group(0)
        
        latex = groups[0].strip()
        try:
            mathml = convert(latex)
            return f'{mathml}'
        except Exception:
            return m.group(0)


    return pattern.sub(repl, text)

def convert_text_to_html(text):
    lines = text.strip().split('\n')
    html_lines = []

    for line in lines:
        html_lines.append(f'<p>{line}</p>')

    return ''.join(html_lines)

def convert_text2html(input_data: str):
    text_mathml = latex_to_inline_mathml(input_data)
    html = convert_text_to_html(text_mathml)
    return html
```

---
File: postprocessing/raw_response.py
---
```python
import re
from typing import Any, Dict, Tuple, Union, Optional


def find_last_tag_block(text: str, tag_name: str) -> Optional[str]:
    """
    Finds the content of the last block between <tag_name>...</tag_name> tags.
    Case-insensitive, supports tags with attributes.
    Returns the content between the last matching tags, or None if not found.
    """
    # Escape tag name for regex
    escaped = re.escape(tag_name)
    open_pattern = re.compile(rf"<\s*{escaped}\b[^>]*>", re.IGNORECASE)
    close_pattern = re.compile(rf"</\s*{escaped}\s*>", re.IGNORECASE)

    # Find all closing tags
    closes = list(close_pattern.finditer(text))
    if not closes:
        return None

    last_close = closes[-1]
    close_start = last_close.start()

    # Find opening tags before last close
    opens = []
    for m in open_pattern.finditer(text):
        if m.start() < close_start:
            opens.append(m)
        else:
            break

    if not opens:
        return None

    last_open = opens[-1]
    content_start = last_open.end()
    return text[content_start:close_start].strip()


def extract_and_remove_thinking_block(text: str) -> Tuple[str, str]:
    """
    Extracts the content of the outermost <thinking>...</thinking> block.
    Returns a tuple: (text_without_block, inner_content)
    """
    # Find all closing thinking tags
    close_pattern = re.compile(r"</thinking\s*>", re.IGNORECASE)
    closes = list(close_pattern.finditer(text))
    if not closes:
        return text, ""

    last_close = closes[-1]
    close_start = last_close.start()
    close_end = last_close.end()

    # Find first opening thinking tag
    open_pattern = re.compile(r"<thinking[^>]*>", re.IGNORECASE)
    open_match = open_pattern.search(text)
    if not open_match or open_match.start() >= close_start:
        return text, ""

    open_start = open_match.start()
    open_end = open_match.end()

    inner = text[open_end:close_start]
    outside = text[:open_start] + text[close_end:]
    return outside, inner


class ExtractedResponse:
    def __init__(self, thinking_block: str, document: Optional[str]):
        self.thinking_block = thinking_block
        self.document = document


def extract_response(text: str) -> ExtractedResponse:
    """
    Extracts the thinking block and the final document content from raw text.
    Returns an ExtractedResponse with thinking_block and document.
    """
    cleaned, thinking = extract_and_remove_thinking_block(text)
    block = find_last_tag_block(cleaned, "assessmentmarkuplanguage")
    doc: Optional[str] = None
    doc = block
    return ExtractedResponse(thinking_block=thinking, document=doc)


def replace_image_tags(
    content: str,
    image_dict: dict[str, str]
) -> Tuple[str, str]:
    """
    Replace <graphic tag='IMx' label='...'> with <img src='...'
    alt='...'/> using URLs from image_dict and preserving label from original tag.
    """
    if not isinstance(image_dict, dict):
        return content, 404

    pattern = re.compile(
        r"<graphic\s+tag=['\"]?(IM[0-9O]+)['\"]?"         # group 1: tag
        r"(?:\s+label=['\"](.*?)['\"])?\s*/?>",           # group 2: optional label
        re.IGNORECASE
    )

    used = set()
    missing = False
    extra = False

    def normalize_key(raw: str) -> str:
        return raw.upper().replace('O', '0')

    def repl(match: re.Match) -> str:
        nonlocal missing, extra
        raw_key = match.group(1)
        label = match.group(2)
        key = normalize_key(raw_key)

        url = image_dict.get(key)
        if not url:
            extra = True
            return ''  # ho·∫∑c match.group(0) ƒë·ªÉ gi·ªØ nguy√™n n·∫øu mu·ªën

        used.add(key)
        alt = label or key
        return f'<img src="{url}" alt="{alt}"/>'

    try:
        new_content = pattern.sub(repl, content)
    except Exception:
        return content, 404

    # Ki·ªÉm tra kh√≥a kh√¥ng d√πng t·ªõi
    dict_keys = {normalize_key(k) for k in image_dict.keys()}
    if dict_keys - used:
        missing = True

    status = 200 if not (missing or extra) else 404
    return new_content, status


if __name__ == "__main__":
    # Example usage
    sample = "<thinking>Compute something</thinking>..."
    res = extract_response(sample)
    print(res.thinking_block, res.document)
```

---
File: postprocessing/replaceimgfig.py
---
```python
import re
from typing import List, Dict, Tuple, Any

def replace_img_to_fig(html: str) -> Tuple[str, List[Dict[str, str]]]:
    image_srcs = []
    index = 0

    def replacer(match):
        nonlocal index
        full_tag = match.group(0)  # to√†n b·ªô th·∫ª <img ...>
        src = match.group(1)
        attrs = match.group(2) or ""

        img_id = f"fig{index}"
        image_srcs.append({"id": img_id, "src": src})
        index += 1

        # Thay th·∫ø b·∫±ng <figure id="figX" ... />
        return f'<figure id="{img_id}" {attrs.strip()} />'

    pattern = r'<img\s+[^>]*src=["\']([^"\']+)["\']([^>]*)>'
    text = re.sub(pattern, replacer, html, flags=re.IGNORECASE)

    return text, image_srcs
def replace_fig2img_immutable(json_obj: Any, figures_data: List[Dict[str, str]]) -> Any:
    """
    T·∫°o m·ªôt b·∫£n sao c·ªßa ƒë·ªëi t∆∞·ª£ng JSON v√† thay th·∫ø c√°c gi√° tr·ªã 'figX' b·∫±ng URL th·ª±c t·∫ø.
    ƒê√¢y l√† ph∆∞∆°ng ph√°p "b·∫•t bi·∫øn" (immutable) - kh√¥ng l√†m thay ƒë·ªïi ƒë·ªëi t∆∞·ª£ng ƒë·∫ßu v√†o.

    Args:
        json_obj: ƒê·ªëi t∆∞·ª£ng JSON (dict ho·∫∑c list) c·∫ßn x·ª≠ l√Ω.
        figures_data: Danh s√°ch c√°c dictionary ch·ª©a mapping {'id': 'figX', 'src': 'url'}.

    Returns:
        M·ªôt ƒë·ªëi t∆∞·ª£ng JSON m·ªõi ƒë√£ ƒë∆∞·ª£c thay th·∫ø.
    """
    # 1. T·∫°o m·ªôt b·∫£ng tra c·ª©u (lookup table) ƒë·ªÉ truy c·∫≠p nhanh id -> src
    id_to_src_map = {figure['id']: figure['src'] for figure in figures_data}

    # 2. ƒê·ªãnh nghƒ©a m·ªôt h√†m ƒë·ªá quy ƒë·ªÉ duy·ªát v√† x√¢y d·ª±ng l·∫°i ƒë·ªëi t∆∞·ª£ng
    def _walk_and_rebuild(node: Any) -> Any:
        # N·∫øu node l√† m·ªôt dictionary...
        if isinstance(node, dict):
            new_dict = {}
            for key, value in node.items():
                # Ki·ªÉm tra n·∫øu ƒë√¢y l√† key 'figures' v√† gi√° tr·ªã l√† chu·ªói c·∫ßn thay th·∫ø
                if key == 'figures' and isinstance(value, str):
                    fig_ids = [i.strip() for i in value.split(',')]
                    
                    # L·∫•y c√°c URL t∆∞∆°ng ·ª©ng, b·ªè qua n·∫øu ID kh√¥ng t·ªìn t·∫°i
                    urls = [id_to_src_map.get(fig_id) for fig_id in fig_ids if id_to_src_map.get(fig_id)]

                    # N·∫øu ch·ªâ c√≥ 1 URL, g√°n tr·ª±c ti·∫øp. N·∫øu nhi·ªÅu, g√°n c·∫£ danh s√°ch.
                    if len(urls) == 1:
                        new_dict[key] = urls[0]
                    elif len(urls) > 1:
                        new_dict[key] = urls
                    else:
                        # N·∫øu kh√¥ng t√¨m th·∫•y URL n√†o, gi·ªØ l·∫°i gi√° tr·ªã g·ªëc
                        new_dict[key] = value 
                else:
                    # N·∫øu kh√¥ng ph·∫£i key 'figures', g·ªçi ƒë·ªá quy cho gi√° tr·ªã c·ªßa n√≥
                    new_dict[key] = _walk_and_rebuild(value)
            return new_dict

        # N·∫øu node l√† m·ªôt list...
        elif isinstance(node, list):
            # G·ªçi ƒë·ªá quy cho t·ª´ng ph·∫ßn t·ª≠ trong list v√† t·∫°o ra m·ªôt list m·ªõi
            return [_walk_and_rebuild(item) for item in node]
        
        # N·∫øu l√† c√°c ki·ªÉu d·ªØ li·ªáu kh√°c (string, number, bool...), tr·∫£ v·ªÅ ch√≠nh n√≥
        else:
            return node

    # 3. B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·ª´ g·ªëc c·ªßa ƒë·ªëi t∆∞·ª£ng JSON
    return _walk_and_rebuild(json_obj)

if __name__ == "__main__":
    figures = [{'id': 'fig0', 'src': 'https://pub-81feb243d1ab4aae8e02911485784de0.r2.dev/a25fdf9b.jpg'}, {'id': 'fig1', 'src': 'https://pub-81feb243d1ab4aae8e02911485784de0.r2.dev/29cccdd3.jpg'}, {'id': 'fig2', 'src': 'https://pub-81feb243d1ab4aae8e02911485784de0.r2.dev/e88ed63b.jpg'}, {'id': 'fig3', 'src': 'https://pub-81feb243d1ab4aae8e02911485784de0.r2.dev/7ae81bb9.jpg'}, {'id': 'fig4', 'src': 'https://pub-81feb243d1ab4aae8e02911485784de0.r2.dev/a1dcd944.jpg'}, {'id': 'fig5', 'src': 'https://pub-81feb243d1ab4aae8e02911485784de0.r2.dev/31d6ba4f.jpg'}]
    with open("/teamspace/studios/this_studio/tests/de15.txt", "r", encoding="utf-8") as f:
        text = f.read()
```

---
File: postprocessing/slurp2json.py
---
```python
import re
import json
from typing import List, Dict, Any, Union, Optional, Literal, TypedDict

# TypeScript interface: Block
# ƒê∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng TypedDict ƒë·ªÉ an to√†n ki·ªÉu
class Block(TypedDict):
    type: str
    content: Union[str, Dict[str, Any]]

# TypeScript interface: State
class State(TypedDict):
    currentBlockType: Optional[str]
    currentBlockObject: Optional[Union[str, Dict[str, Any]]]
    inMultiLine: bool
    multiLineTarget: Optional[Literal['block', 'property']]
    propertyStack: List[str]

class SLURPParser:
    """
    Ph√¢n t√≠ch c√∫ ph√°p vƒÉn b·∫£n ƒë·ªãnh d·∫°ng SLURP th√†nh m·ªôt c·∫•u tr√∫c c√°c kh·ªëi (blocks).
    SLURP l√† m·ªôt ƒë·ªãnh d·∫°ng ƒë∆°n gi·∫£n, th·ª•t ƒë·∫ßu d√≤ng ƒë·ªÉ bi·ªÉu di·ªÖn c√°c kh·ªëi d·ªØ li·ªáu l·ªìng nhau.
    """
    _BLOCK_REGEX = re.compile(r"^(\w+):\s*\|?(.*)$")
    _PROP_REGEX = re.compile(r"^(>+)\s*([\w-]+):\s*\|?(.*)$")

    def __init__(self):
        self.result: List[Block] = []
        self.current_state: State = self._get_initial_state()

    def _get_initial_state(self) -> State:
        """Tr·∫£ v·ªÅ m·ªôt t·ª´ ƒëi·ªÉn tr·∫°ng th√°i ban ƒë·∫ßu."""
        return {
            "currentBlockType": None,
            "currentBlockObject": None,
            "inMultiLine": False,
            "multiLineTarget": None,
            "propertyStack": [],
        }

    def parse(self, text_content: str) -> List[Block]:
        """
        Ph√¢n t√≠ch c√∫ ph√°p m·ªôt chu·ªói ƒë·∫ßu v√†o SLURP v√† tr·∫£ v·ªÅ m·ªôt danh s√°ch c√°c kh·ªëi.
        """
        self.result = []
        self.current_state = self._get_initial_state()

        lines = text_content.replace('\r\n', '\n').split('\n')

        for line in lines:
            if not line.strip() and not self.current_state["inMultiLine"]:
                continue
            self._process_line(line)

        self._finalize_current_block()
        return self.result

    # ========================================================================
    # H√ÄM ƒê√É ƒê∆Ø·ª¢·ª¢C S·ª¨A L·ªñI B√äN D∆Ø·ªöI
    # ========================================================================
    def _process_line(self, line: str) -> None:
        """X·ª≠ l√Ω m·ªôt d√≤ng ƒë∆°n t·ª´ ƒë·∫ßu v√†o SLURP."""
        block_match = self._BLOCK_REGEX.match(line)
        prop_match = self._PROP_REGEX.match(line)

        # Ph√°t hi·ªán kh·ªëi m·ªõi
        if not line.startswith('>') and not line.startswith(' ') and block_match:
            self._finalize_current_block()

            block_type, value = block_match.groups()
            self.current_state["currentBlockType"] = block_type
            trimmed_value = value.strip()

            if trimmed_value or '|' in line:
                self.current_state["currentBlockObject"] = trimmed_value
                if '|' in line:
                    self.current_state["inMultiLine"] = True
                    self.current_state["multiLineTarget"] = 'block'
            else:
                self.current_state["currentBlockObject"] = {}
                self.current_state["inMultiLine"] = False

        # Ph√°t hi·ªán thu·ªôc t√≠nh
        elif prop_match:
            self.current_state["inMultiLine"] = False

            gt, key, value = prop_match.groups()
            level = len(gt)

            # C·∫Øt ng·∫Øn ngƒÉn x·∫øp thu·ªôc t√≠nh v·ªÅ m·ª©c ƒë·ªô th·ª•t ƒë·∫ßu d√≤ng hi·ªán t·∫°i
            self.current_state["propertyStack"] = self.current_state["propertyStack"][:level - 1]

            parent_obj = self.current_state["currentBlockObject"]
            if not isinstance(parent_obj, dict):
                # T·ª± ƒë·ªông chuy·ªÉn ƒë·ªïi chu·ªói th√†nh ƒë·ªëi t∆∞·ª£ng n·∫øu c·∫ßn
                parent_obj = {}
                self.current_state["currentBlockObject"] = parent_obj

            # *** PH·∫¶N S·ª¨A L·ªñI QUAN TR·ªåNG ***
            # ƒêo·∫°n code n√†y ƒë·∫£m b·∫£o r·∫±ng ch√∫ng ta ƒëang ƒëi v√†o m·ªôt t·ª´ ƒëi·ªÉn (dict).
            # N·∫øu ƒë∆∞·ªùng d·∫´n t·ªìn t·∫°i nh∆∞ng ch·ª©a m·ªôt gi√° tr·ªã kh√¥ng ph·∫£i dict (v√≠ d·ª•: m·ªôt chu·ªói r·ªóng),
            # n√≥ s·∫Ω ƒë∆∞·ª£c ghi ƒë√® b·∫±ng m·ªôt dict r·ªóng.
            current_level_ref = parent_obj
            for path_key in self.current_state["propertyStack"]:
                # Ki·ªÉm tra xem kh√≥a c√≥ t·ªìn t·∫°i kh√¥ng v√† gi√° tr·ªã c·ªßa n√≥ c√≥ ph·∫£i l√† dict kh√¥ng
                if not isinstance(current_level_ref.get(path_key), dict):
                    # N·∫øu kh√¥ng, t·∫°o/ghi ƒë√® n√≥ b·∫±ng m·ªôt dict r·ªóng
                    current_level_ref[path_key] = {}
                # ƒêi s√¢u v√†o c·∫•p ƒë·ªô ti·∫øp theo
                current_level_ref = current_level_ref[path_key]
            # *** K·∫æT TH√öC PH·∫¶N S·ª¨A L·ªñI ***

            # B√¢y gi·ªù `current_level_ref` ch·∫Øc ch·∫Øn l√† m·ªôt dict
            current_level_ref[key] = value.strip()
            self.current_state["propertyStack"].append(key)

            if '|' in line:
                self.current_state["inMultiLine"] = True
                self.current_state["multiLineTarget"] = 'property'

        # X·ª≠ l√Ω n·ªôi dung nhi·ªÅu d√≤ng
        elif self.current_state["inMultiLine"]:
            content = line
            if self.current_state["multiLineTarget"] == 'block':
                if isinstance(self.current_state["currentBlockObject"], str):
                    self.current_state["currentBlockObject"] += '\n' + content
            elif self.current_state["multiLineTarget"] == 'property':
                parent = self.current_state["currentBlockObject"]
                if isinstance(parent, dict):
                    stack = self.current_state["propertyStack"]
                    if not stack: return # B·ªè qua n·∫øu kh√¥ng c√≥ thu·ªôc t√≠nh n√†o trong ngƒÉn x·∫øp

                    # ƒêi ƒë·∫øn cha c·ªßa thu·ªôc t√≠nh cu·ªëi c√πng
                    for i in range(len(stack) - 1):
                        # Gi·∫£ s·ª≠ ƒë∆∞·ªùng d·∫´n ƒë√£ h·ª£p l·ªá v√¨ n√≥ ƒë∆∞·ª£c t·∫°o ·ªü tr√™n
                        parent = parent[stack[i]]

                    final_key = stack[-1]
                    if final_key in parent and isinstance(parent[final_key], str):
                        parent[final_key] += '\n' + content
    # ========================================================================
    # K·∫æT TH√öC H√ÄM ƒê√É ƒê∆Ø·ª¢C S·ª¨A L·ªñI
    # ========================================================================

    def _finalize_current_block(self) -> None:
        """L∆∞u kh·ªëi hi·ªán ƒëang ƒë∆∞·ª£c x·ª≠ l√Ω v√†o k·∫øt qu·∫£."""
        current_block_type = self.current_state["currentBlockType"]
        current_block_object = self.current_state["currentBlockObject"]

        if current_block_type and current_block_object is not None:
            final_content = current_block_object

            if isinstance(final_content, str):
                final_content = final_content.strip()

            self.result.append({
                "type": current_block_type,
                "content": final_content
            })

        self.current_state = self._get_initial_state()


def format_output(parsed_array: List[Block]) -> str:
    """
    Chuy·ªÉn ƒë·ªïi danh s√°ch c√°c kh·ªëi ƒë√£ ph√¢n t√≠ch c√∫ ph√°p th√†nh ƒë·ªãnh d·∫°ng CSV ph·∫≥ng t√πy ch·ªânh.
    ƒê·ªãnh d·∫°ng: blockType[index].path.to.property,"value"
    """
    output_lines: List[str] = []
    counters: Dict[str, int] = {}

    def flatten(obj: Dict[str, Any], path: str):
        for key, value in obj.items():
            new_path = f"{path}.{key}" if path else key
            if value and isinstance(value, dict):
                flatten(value, new_path)
            else:
                # json.dumps tr√™n m·ªôt chu·ªói s·∫Ω th√™m d·∫•u ngo·∫∑c k√©p v√† tho√°t c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
                formatted_value = json.dumps(str(value))
                output_lines.append(f"{new_path},{formatted_value}")

    for block in parsed_array:
        block_type = block['type']
        block_content = block['content']

        counters[block_type] = counters.get(block_type, 0) + 1
        index = counters[block_type]
        base_path = f"{block_type}[{index}]"

        if block_content and isinstance(block_content, dict):
            flatten(block_content, base_path)
        else:
            formatted_value = json.dumps(str(block_content))
            output_lines.append(f"{base_path},{formatted_value}")

    return '\n'.join(output_lines)


def split_csv_lines_safe(input_str: str) -> List[str]:
    """
    T√°ch c√°c d√≤ng CSV m·ªôt c√°ch an to√†n, x·ª≠ l√Ω c√°c d·∫•u xu·ªëng d√≤ng b√™n trong c√°c chu·ªói ƒë∆∞·ª£c tr√≠ch d·∫´n.
    """
    lines: List[str] = []
    current_line = ""
    inside_quotes = False

    for i, char in enumerate(input_str):
        if char == '"':
            # Trong JS, input[-1] l√† undefined. Trong Python, ƒë√≥ l√† k√Ω t·ª± cu·ªëi c√πng.
            # V√¨ v·∫≠y, ch√∫ng ta ph·∫£i ki·ªÉm tra r√µ r√†ng i > 0.
            is_escaped = i > 0 and input_str[i-1] == '\\'
            if not is_escaped:
                inside_quotes = not inside_quotes

        if char == '\n' and not inside_quotes:
            lines.append(current_line)
            current_line = ""
        else:
            current_line += char

    if current_line:
        lines.append(current_line)

    return lines

# ƒê·ªãnh nghƒ©a c√°c lo·∫°i ƒë·ªÉ l√†m r√µ, t∆∞∆°ng t·ª± nh∆∞ DataObject v√† DataMap trong TypeScript
DataMap = Dict[str, List[Dict[str, Any]]]
_CSV_KEY_REGEX = re.compile(r"^(\w+)\[(\d+)\](?:\.(.+))?$")

def parse_csv_to_json(input_str: str) -> DataMap:
    """
    Ph√¢n t√≠ch c√∫ ph√°p chu·ªói ƒë·ªãnh d·∫°ng CSV t√πy ch·ªânh th√†nh m·ªôt c·∫•u tr√∫c ƒë·ªëi t∆∞·ª£ng l·ªìng nhau.
    """
    lines = split_csv_lines_safe(input_str)
    data: DataMap = {}

    for line in lines:
        if not line:
            continue

        parts = line.split(',', 1)
        if len(parts) < 2:
            continue
        raw_key, raw_value = parts

        try:
            value = json.loads(raw_value)
        except json.JSONDecodeError:
            value = raw_value # D·ª± ph√≤ng n·∫øu kh√¥ng ph·∫£i l√† m·ªôt chu·ªói JSON h·ª£p l·ªá

        match = _CSV_KEY_REGEX.match(raw_key)
        if not match:
            continue

        obj_name, idx_str, path = match.groups()
        index = int(idx_str)

        if obj_name not in data:
            data[obj_name] = []

        # ƒê·∫£m b·∫£o danh s√°ch ƒë·ªß d√†i ƒë·ªÉ ch·ª©a ch·ªâ m·ª•c
        if len(data[obj_name]) < index:
            data[obj_name].extend([None] * (index - len(data[obj_name])))

        list_index = index - 1
        if data[obj_name][list_index] is None:
            data[obj_name][list_index] = {}

        target = data[obj_name][list_index]

        if path:
            keys = path.split('.')
            # ƒêi·ªÅu h∆∞·ªõng/t·∫°o c√°c ƒë·ªëi t∆∞·ª£ng l·ªìng nhau
            for key_part in keys[:-1]:
                target = target.setdefault(key_part, {})
            target[keys[-1]] = value
        else:
            target['value'] = value

    for key in data:
        data[key] = [entry for entry in data[key] if entry is not None]

    return data


def slurp_to_json(slurp: str):
    """
    Ch·ª©c nƒÉng ƒëi·ªÅu ph·ªëi ch√≠nh chuy·ªÉn ƒë·ªïi m·ªôt chu·ªói SLURP th√†nh m·ªôt chu·ªói JSON.
    """
    parser = SLURPParser()
    parsed_blocks = parser.parse(slurp)
    csv_str = format_output(parsed_blocks)
    json_obj = parse_csv_to_json(csv_str)
    return json_obj

# ---- V√≠ d·ª• s·ª≠ d·ª•ng ----
if __name__ == "__main__":
    sample_slurp_input = """
>name: ƒê·ªÄ THI TH·ª¨
>subject: V·∫¨T L√ç
>code: ƒê·ªÄ 15
>duration: 50 ph√∫t

shareinfo:
>id: const
>info: |
\(œÄ = 3,14\); \(T (K) = t (¬∞C) + 273\); \(R = 8,31 J. mol^{-1}. K^{-1}\); \(N_A = 6,02. 10^{23}\) h·∫°t/mol.

sectionHeader: PH·∫¶N I. Th√≠ sinh tr·∫£ l·ªùi t·ª´ c√¢u 1 ƒë·∫øn c√¢u 18. M·ªói c√¢u h·ªèi th√≠ sinh ch·ªâ ch·ªçn m·ªôt ph∆∞∆°ng √°n.

shareinfo:
>id: share-10-11
>info: |
H√¨nh b√™n m√¥ t·∫£ m·ªôt m√°y ph√°t ƒëi·ªán xoay chi·ªÅu ƒë∆°n gi·∫£n. M√°y ph√°t ƒëi·ªán xoay chi·ªÅu g·ªìm hai b·ªô ph·∫≠n ch√≠nh l√† ph·∫ßn c·∫£m v√† ph·∫ßn ·ª©ng.
>figures: fig0

qs:
>dnum: 10
>type: mcq
>shared-info: share-10-11
>qt: M√°y ph√°t ƒëi·ªán ho·∫°t ƒë·ªông d·ª±a tr√™n
>labels:
>>a: hi·ªán t∆∞·ª£ng c·∫£m ·ª©ng ƒëi·ªán t·ª´.
>>b: hi·ªán t∆∞·ª£ng t√≠ch ƒëi·ªán.
>>c: hi·ªán t∆∞·ª£ng quang ƒëi·ªán.
>>d: hi·ªán t∆∞·ª£ng nhi·ªÖm ƒëi·ªán do c·ªç x√°t.

qs:
>dnum: 11
>type: mcq
>shared-info: share-10-11
>qt: Ph·∫ßn c·∫£m t·∫°o ra ...(1)..., ph·∫ßn ·ª©ng t·∫°o ra ...(2)... khi m√°y ho·∫°t ƒë·ªông. T·ª´ th√≠ch h·ª£p ƒëi·ªÅn v√†o v·ªã tr√≠ (1) v√† (2) l·∫ßn l∆∞·ª£t l√†
>labels:
>>a: T·ª´ tr∆∞·ªùng, su·∫•t ƒëi·ªán ƒë·ªông c·∫£m ·ª©ng.
>>b: D√≤ng ƒëi·ªán, t·ª´ tr∆∞·ªùng.
>>c: Su·∫•t ƒëi·ªán ƒë·ªông c·∫£m ·ª©ng, t·ª´ tr∆∞·ªùng.
>>d: Su·∫•t ƒëi·ªán ƒë·ªông c·∫£m ·ª©ng, d√≤ng ƒëi·ªán.

qs:
>dnum: 12
>type: mcq
>qt: M·ªôt d√¢y d·∫´n th·∫≥ng d√†i v√¥ h·∫°n c√≥ ph∆∞∆°ng vu√¥ng g√≥c v·ªõi m·∫∑t ph·∫≥ng trang gi·∫•y. Cho d√≤ng ƒëi·ªán ch·∫°y qua d√¢y d·∫´n theo chi·ªÅu t·ª´ trong ra ngo√†i. H√¨nh n√†o d∆∞·ªõi ƒë√¢y m√¥ t·∫£ ƒë√∫ng ƒë∆∞·ªùng s·ª©c t·ª´ tr√™n m·∫∑t ph·∫≥ng trang gi·∫•y c·ªßa t·ª´ tr∆∞·ªùng c·ªßa d√≤ng ƒëi·ªán ch·∫°y trong d√¢y d·∫´n?
>figures: fig1, fig2, fig3
>labels:
>>a: H√¨nh 2.
>>b: H√¨nh 3.
>>c: H√¨nh 4.
>>d: H√¨nh 1.

sectionHeader: PH·∫¶N II. Th√≠ sinh tr·∫£ l·ªùi t·ª´ c√¢u 1 ƒë·∫øn c√¢u 4. Trong m·ªói √Ω a), b), c), d) ·ªü m·ªói c√¢u, th√≠ sinh ch·ªçn ƒë√∫ng ho·∫∑c sai.

qs:
>dnum: 3
>type: mtf-2018
>qt: M·ªôt thanh kim lo·∫°i c√≥ kh·ªëi l∆∞·ª£ng m = 50 g c√≥ th·ªÉ tr∆∞·ª£t v·ªõi ma s√°t kh√¥ng ƒë√°ng k·ªÉ tr√™n hai thanh ray song song n·∫±m ngang c√°ch nhau m·ªôt kho·∫£ng d = 4 cm. ƒê∆∞·ªùng ray n·∫±m trong m·ªôt t·ª´ tr∆∞·ªùng ƒë·ªÅu th·∫≥ng ƒë·ª©ng c√≥ ƒë·ªô l·ªõn B = 0,3 T v√† c√≥ h∆∞·ªõng nh∆∞ h√¨nh b√™n. T·∫°i th·ªùi ƒëi·ªÉm t = 0 s, ƒëi·ªán k·∫ø G ƒë∆∞·ª£c k·∫øt n·ªëi v·ªõi thanh ray, t·∫°o ra d√≤ng ƒëi·ªán kh√¥ng ƒë·ªïi I = 2 A (c√≥ chi·ªÅu nh∆∞ h√¨nh) trong d√¢y v√† thanh ray (k·ªÉ c·∫£ khi d√¢y chuy·ªÉn ƒë·ªông). Bi·∫øt ban ƒë·∫ßu thanh ƒë·ª©ng y√™n.
>figures: fig4
>labels:
>>a: L·ª±c t√°c d·ª•ng l√™n thanh l√† l·ª±c t·ª´.
>>b: T·ª´ tr∆∞·ªùng do d√≤ng ƒëi·ªán t·∫°o ra c√≥ h∆∞·ªõng h∆∞·ªõng theo chi·ªÅu nh∆∞ t·ª´ tr∆∞·ªùng b√™n ngo√†i.
>>c: Thanh kim lo·∫°i chuy·ªÉn ƒë·ªông sang tr√°i v√† t·∫°i l√∫c t = 1 s v·∫≠n t·ªëc c·ªßa thanh c√≥ ƒë·ªô l·ªõn l√† 0,48 m/s.
>>d: Qu√£ng ƒë∆∞·ªùng thanh ƒëi ƒë∆∞·ª£c sau th·ªùi gian 2 s k·ªÉ t·ª´ l√∫c thi·∫øt b·ªã G ƒë∆∞·ª£c k·∫øt n·ªëi l√† 0,48 m.

qs:
>dnum: 4
>type: mtf-2018
>qt: Trong y h·ªçc m·ªôt ƒë·ªìng v·ªã ph√≥ng x·∫° c·ªßa Sodium th∆∞·ªùng ƒë∆∞·ª£c d√πng ƒë·ªÉ x√°c ƒë·ªãnh l∆∞·ª£ng m√°u trong c∆° th·ªÉ ng∆∞·ªùi l√† \(^{24}_{11}Na\). Chu k·ª≥ b√°n r√£ c·ªßa \(^{24}_{11}Na\) l√† 15 gi·ªù. Ng∆∞·ªùi ta l·∫•y m·ªôt l∆∞·ª£ng \(^{24}_{11}Na\) c√≥ ƒë·ªô ph√≥ng x·∫° 2,5 \(\mu\)Ci ƒë·ªÉ ti√™m v√†o m·ªôt b·ªánh nh√¢n. Sau 3 gi·ªù, h·ªç l·∫•y ra 1 cm¬≥ m√°u t·ª´ ng∆∞·ªùi ƒë√≥ th√¨ th·∫•y n√≥ c√≥ 145 ph√¢n r√£ trong 10 gi√¢y. Cho bi·∫øt ƒë·ªìng v·ªã \(^{24}_{11}Na\) ph√≥ng x·∫° t·∫°o ra \(^{24}_{12}Mg\).
>labels:
>>a: ƒê√¢y l√† ph√¢n r√£ \(\beta^+\).
>>b: ƒê·ªô ph√≥ng x·∫° l√∫c m·ªõi ti√™m v√†o c∆° th·ªÉ ng∆∞·ªùi l√† 7,4 ¬∑ \(10^4\) Bq.
>>c: S·ªë nguy√™n t·ª≠ \(^{24}_{11}Na\) trong 1 cm¬≥ m√°u sau 3 gi·ªù l√† 3 ¬∑ \(10^5\) nguy√™n t·ª≠.
>>d: Th·ªÉ t√≠ch m√°u c·ªßa ng∆∞·ªùi ƒë√≥ l√† 5,6 l√≠t.

sectionHeader: PH·∫¶N III. Th√≠ sinh tr·∫£ l·ªùi t·ª´ c√¢u 1 ƒë·∫øn c√¢u 6.

shareinfo:
>id: share-1-2
>info: |
Trong m·ªôt h·ªá th·ªëng ƒëun n∆∞·ªõc b·∫±ng nƒÉng l∆∞·ª£ng m·∫∑t tr·ªùi, √°nh s√°ng M·∫∑t Tr·ªùi ƒë∆∞·ª£c h·∫•p th·ª• b·ªüi n∆∞·ªõc ch·∫£y qua c√°c ·ªëng trong m·ªôt b·ªô thu nhi·ªát tr√™n m√°i nh√†. √Ånh s√°ng M·∫∑t Tr·ªùi ƒëi qua l·ªõp k√≠nh trong su·ªët c·ªßa b·ªô thu v√† l√†m n√≥ng n∆∞·ªõc trong ·ªëng. Sau ƒë√≥, n∆∞·ªõc n√≥ng n√†y ƒë∆∞·ª£c b∆°m v√†o b·ªÉ ch·ª©a. Bi·∫øt nhi·ªát dung ri√™ng c·ªßa n∆∞·ªõc l√† \(c_{H_2O}\) = 4200 J¬∑\(kg^{-1}\)¬∑ \(K^{-1}\), kh·ªëi l∆∞·ª£ng ri√™ng c·ªßa n∆∞·ªõc l√† \(D_{H_2O}\) = 1000 kg/m¬≥ .

qs:
>dnum: 1
>type: short-2018
>shared-info: share-1-2
>qt: Bi·∫øt r·∫±ng s·ª± t·ªèa nhi·ªát c·ªßa h·ªá th·ªëng ra kh√¥ng kh√≠ l√† kh√¥ng ƒë√°ng k·ªÉ. NƒÉng l∆∞·ª£ng c·∫ßn thi·∫øt ƒë·ªÉ l√†m n√≥ng 2 l√≠t n∆∞·ªõc t·ª´ 20¬∞C ƒë·∫øn 100¬∞C l√† x ¬∑ \(10^6\) J. T√¨m x (l√†m tr√≤n k·∫øt qu·∫£ ƒë·∫øn ch·ªØ s·ªë h√†ng ph·∫ßn trƒÉm).

qs:
>dnum: 2
>type: short-2018
>shared-info: share-1-2
>qt: Th·ª±c t·∫ø h·ªá th·ªëng ch·ªâ ho·∫°t ƒë·ªông v·ªõi hi·ªáu su·∫•t 30%, n√™n ch·ªâ 30% nƒÉng l∆∞·ª£ng M·∫∑t Tr·ªùi ƒë∆∞·ª£c d√πng ƒë·ªÉ l√†m n√≥ng n∆∞·ªõc. ƒê·ªÉ l√†m n√≥ng 2 l√≠t n∆∞·ªõc t·ª´ 20¬∞C ƒë·∫øn 100¬∞C th√¨ ph·∫£i cung c·∫•p nhi·ªát trong th·ªùi gian t. Bi·∫øt r·∫±ng c∆∞·ªùng ƒë·ªô √°nh s√°ng M·∫∑t Tr·ªùi chi·∫øu xu·ªëng l√† I = 1000 W¬∑\(m^{-2}\), di·ªán t√≠ch c·ªßa b·ªô thu l√† S = 3 m¬≤. C√¥ng su·∫•t b·ª©c x·∫° nhi·ªát chi·∫øu l√™n b·ªô thu nhi·ªát ƒë∆∞·ª£c cho b·ªüi c√¥ng th·ª©c sau: P = I ¬∑ S. T√≠nh t theo ƒë∆°n v·ªã ph√∫t (l√†m tr√≤n k·∫øt qu·∫£ ƒë·∫øn ch·ªØ s·ªë h√†ng ƒë∆°n v·ªã).
"""

    json_output = slurp_to_json(sample_slurp_input)
    print(json_output)
```

---
File: prompt/__init__.py
---
```python
import re
import os
from google.genai import types
BASE_DIR = os.path.dirname(__file__)  
# T∆∞∆°ng ƒë∆∞∆°ng v·ªõi readTextFile
def read_text_file(dir_path, file_path):
    with open(os.path.join(dir_path, file_path), 'r', encoding='utf-8') as file:
        return file.read()

def process_examples_xml(content: str):
    matches = re.findall(r"<example>([\s\S]*?)<\/example>", content)
    examples = []

    for match in matches:
        user_query_match = re.search(r"<user_query>([\s\S]*?)<\/user_query>", match)
        assistant_response_match = re.search(r"<assistant_response>([\s\S]*?)<\/assistant_response>", match)

        user_query = user_query_match.group(1).strip() if user_query_match else ''
        assistant_response = assistant_response_match.group(1).strip() if assistant_response_match else ''

        examples.append({
            'user_query': user_query,
            'assistant_response': assistant_response
        })

    # Mapping to contents
    contents = []
    for example in examples:
        contents.append(
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(text=example['user_query']),
                ]
            )
        )

        contents.append(
            types.Content(
                role="model",
                parts=[
                    types.Part.from_text(text=example['assistant_response']),
                ]
            )
        )


    return contents

# Initial Slurp
def get_initial_slurp_prompt():
    return read_text_file(BASE_DIR, './convert2slurp/initial_slurp.md')

def get_initial_slurp_examples():
    content = read_text_file(BASE_DIR, './convert2slurp/initial_example.xml')
    return process_examples_xml(content)

# Continuation Prompt
def get_slurp_continuation_prompt():
    return read_text_file(BASE_DIR, './convert2slurp/continuation_slurp.md')

def get_slurp_continuation_examples():
    initial_example = process_examples_xml(read_text_file(BASE_DIR, './convert2slurp/initial_example.xml'))
    cont_example = process_examples_xml(read_text_file(BASE_DIR, './convert2slurp/continuation_example.xml'))
    return initial_example + cont_example

# Extraction Prompts
def get_extraction_figure_prompt():
    return read_text_file(BASE_DIR, './extraction/figure.txt')

def get_extraction_non_figure_prompt():
    return read_text_file(BASE_DIR, './extraction/non_docfigure.txt')
```

---
File: prompt/__pycache__/__init__.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xff in position 9: invalid start byte

---
File: prompt/convert2slurp/continuation_example.xml
---
```xml
<examples>
<example>
<user_query>
@mml_start@C√¢u 1. M·∫∑t tr·∫≠n Vi·ªát Minh ƒë∆∞·ª£c th√†nh l·∫≠p v√†o th·ªùi gian n√†o?
A. 3/2/1930
B. 22/12/1944
C. 19/5/1941
D. 2/9/1945
C√¢u 2. T·ªï ch·ª©c ASEAN ƒë∆∞·ª£c th√†nh l·∫≠p nƒÉm n√†o?
A. 1965
B. 1967
C. 1970
D. 1975@mml_end@

@slurp_incomplete_start@qs:
>dnum: 1
>type: mcq
>qt: M·∫∑t tr·∫≠n Vi·ªát Minh ƒë∆∞·ª£c th√†nh l·∫≠p v√†o th·ªùi gian n√†o?
>items:
>>a: 3/2/1930
>>b: 22/12/1944
>>c: 19/5/1941
>>d: 2/9/1945
qs:
>dnum: 2
>type: mcq@slurp_incomplete_end@
</user_query>


<assistant_response>
@slurp_resume_start@>qt: T·ªï ch·ª©c ASEAN ƒë∆∞·ª£c th√†nh l·∫≠p nƒÉm n√†o?
>items:
>>a: 1965
>>b: 1967
>>c: 1970
>>d: 1975@slurp_resume_end@
</assistant_response>
</example>


<example>


<user_query>
@mml_start@ƒê·ªçc th√¥ng tin sau v√† tr·∫£ l·ªùi c√¢u h·ªèi 1 v√† 2:

Quang h·ª£p l√† qu√° tr√¨nh th·ª±c v·∫≠t s·ª≠ d·ª•ng √°nh s√°ng m·∫∑t tr·ªùi ƒë·ªÉ t·ªïng h·ª£p c√°c ch·∫•t h·ªØu c∆° t·ª´ CO‚ÇÇ v√† H‚ÇÇO, gi·∫£i ph√≥ng O‚ÇÇ.

C√¢u 1. Qu√° tr√¨nh quang h·ª£p x·∫£y ra ·ªü b·ªô ph·∫≠n n√†o c·ªßa c√¢y?
A. R·ªÖ
B. Th√¢n
C. L√°
D. Hoa

C√¢u 2. S·∫£n ph·∫©m ch√≠nh c·ªßa qu√° tr√¨nh quang h·ª£p l√† g√¨?
A. N∆∞·ªõc
B. CO‚ÇÇ
C. Ch·∫•t h·ªØu c∆°
D. √Ånh s√°ng@mml_end@

@slurp_incomplete_start@shareinfo:
>id: s-photosynthesis
>info: Quang h·ª£p l√† qu√° tr√¨nh th·ª±c v·∫≠t s·ª≠ d·ª•ng √°nh s√°ng m·∫∑t tr·ªùi ƒë·ªÉ t·ªïng h·ª£p c√°c ch·∫•t h·ªØu c∆° t·ª´ CO‚ÇÇ v√† H‚ÇÇO, gi·∫£i ph√≥ng O‚ÇÇ.
qs:
>dnum: 1
>type: mcq
>shared-info: s-photosynthesis
>qt: Qu√° tr√¨nh quang h·ª£p x·∫£y ra ·ªü b·ªô ph·∫≠n n√†o c·ªßa c√¢y?
>items:
>>a: R·ªÖ
>>b: Th√¢n
>@slurp_incomplete_end@
</user_query>



<assistant_response>
@slurp_resume_start@>c: L√°
>>d: Hoa
qs:
>dnum: 2
>type: mcq
>shared-info: s-photosynthesis
>qt: S·∫£n ph·∫©m ch√≠nh c·ªßa qu√° tr√¨nh quang h·ª£p l√† g√¨?
>items:
>>a: N∆∞·ªõc
>>b: CO‚ÇÇ
>>c: Ch·∫•t h·ªØu c∆°
>>d: √Ånh s√°ng@slurp_resume_end@
</assistant_response>


</example>


<example>


<user_query>
@mml_start@Mark the letter A, B, C or D on your answer sheet to indicate the word whose underlined part differs from the others in pronunciation in each of the following questions.

Question 1: A. chemistry  B. chicken  C. change  D. choose
Question 2: A. laughed   B. passed   C. played   D. watched@mml_end@
@slurp_incomplete_start@sectionHeader: Mark the letter A, B, C or D on your answer sheet to indicate the word whose underlined part differs from the others in pronunciation in each of the following questions.
qs:
>dnum: 1
>type: mcq
>items:
>>a: chemistry
>>b: chicken
>>c:@slurp_incomplete_end@
</user_query>


<assistant_response>
@slurp_resume_start@ change
>>d: choose
qs:
>dnum: 2
>type: mcq
>items:
>>a: laughed
>>b: passed
>>c: played
>>d: watched@slurp_resume_end@
</assistant_response>


</example>
</examples>
```

---
File: prompt/convert2slurp/continuation_slurp.md
---
```
# [System Config] M√¥ t·∫£ vai tr√≤ & tr√°ch nhi·ªám
Role: B·∫°n l√† m·ªôt m√¥ h√¨nh chuy√™n x·ª≠ l√Ω chuy·ªÉn ƒë·ªïi ƒë·ªÅ thi,c√≥ nhi·ªám v·ª• bi√™n d·ªãch user_query t·ª´ ƒë·ªãnh d·∫°ng Domain Specific Language (DSL) l√† MML (Minimal Markup Language) m·ªôt ƒë·ªãnh d·∫°ng DSL kh√°c l√† SLURP.

## [Operational Mode] ‚Äî Ch·∫ø ƒë·ªô ho·∫°t ƒë·ªông
### Kh·ªüi t·∫°o t·ª´ ƒë·∫ßu
#### ƒê·∫ßu V√†o
- MML: To√†n b·ªô n·ªôi dung ƒë·ªÅ thi g·ªëc (user_query) ƒë∆∞·ª£c b·ªçc trong `@mml_start@` - `@mml_end@`
#### Quy Tr√¨nh Alpha
1. Ph√¢n t√≠ch c·∫•u tr√∫c
   T·ª± ƒë·ªông ph√°t hi·ªán c√°c kh·ªëi n·ªôi dung: th√¥ng tin ƒë·ªÅ, ti√™u ƒë·ªÅ ph·∫ßn, ƒëo·∫°n d·∫´n chung, c√¢u h·ªèi (v√† n·ªôi dung c√¢u h·ªèi v√† c√°c m·ªánh ƒë·ªÅ/ l·ª±a ch·ªçn/ √Ω ph·ª•).
2. G·∫Øn nh√£n & ph√¢n lo·∫°i
   G√°n nh√£n kh·ªëi ƒë∆∞·ª£c ph√°t hi·ªán v√†o m·ªôt trong c√°c ƒë·ªëi t∆∞·ª£ng sau
   * `meta`, `sectionHeader`, `shareinfo`, `qs`
3. Xu·∫•t k·∫øt qu·∫£
   Bao to√†n b·ªô n·ªôi dung trong c·∫∑p `@slurp_start@` - `@slurp_end@`.
#### ƒê·∫ßu Ra
@slurp_start@[user_query ch·ª©a MML ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi th√†nh SLURP]@slurp_end@
###  Ch·∫ø ƒë·ªô ti·∫øp t·ª•c (resume mode):
#### ƒê·∫ßu V√†o
- AML: N·ªôi dung ƒë·ªÅ thi g·ªëc 
- SLURP Incomplete: SLURP ƒë√£ ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi tr∆∞·ªõc ƒë√≥ t∆∞∆°ng ·ª©ng v·ªõi AML

#### Quy Tr√¨nh Beta
1. Ph√¢n t√≠ch ƒëi·ªÉm d·ª´ng: T·ª± ƒë·ªông ƒë·ªãnh v·ªã ƒëo·∫°n cu·ªëi ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω trong SLURP Incomplete, ƒë·ªëi chi·∫øu v·ªã tr√≠ ƒë√≥ v·ªõi n·ªôi dung t∆∞∆°ng ·ª©ng trong AML.
2. Ti·∫øp t·ª•c chuy·ªÉn ƒë·ªïi: B·∫Øt ƒë·∫ßu x·ª≠ l√Ω t·ª´ v·ªã tr√≠ ƒë√£ d·ª´ng, √°p d·ª•ng c√πng quy t·∫Øc nh∆∞ trong quy tr√¨nh Alpha.
3. Xu·∫•t k·∫øt qu·∫£
   Bao to√†n b·ªô n·ªôi dung trong c·∫∑p `@slurp_resume_start@` - `@slurp_resume_end@`.
#### ƒê·∫ßu Ra
@slurp_resume_start@[user_query ch·ª©a MML ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi th√†nh SLURP]@slurp_resume_end@
QUAN TR·ªåNG: 
* Ngay l·∫≠p t·ª©c b·∫Øt ƒë·∫ßu t·ª´ n∆°i b·∫°n ƒë√£ d·ª´ng l·∫°i m√† kh√¥ng b·ªã gi√°n ƒëo·∫°n.
* Kh√¥ng l·∫∑p l·∫°i b·∫•t k·ª≥ n·ªôi dung n√†o t·ª´ ph·∫ßn x·ª≠ l√Ω tr∆∞·ªõc.

## [Content Constraints] Nh·ªØng ƒëi·ªÅu b·∫Øt bu·ªôc v√† b·ªã c·∫•m

### ƒê∆Ø·ª¢C PH√âP
- Bi·∫øn ƒë·ªïi MML th√†nh ƒë·ªãnh d·∫°ng SLURP c√≥ c·∫•u tr√∫c
- Format l·∫°i c√¥ng th·ª©c to√°n t·ª´ c√°c ki·ªÉu `$$...$$`,`$...$`,.. th√†nh `\(...\)`
- C·∫•u tr√∫c h√≥a n·ªôi dung tu·∫ßn t·ª± gi·ªëng nh∆∞ trong ƒë·ªÅ g·ªëc
### TUY·ªÜT ƒê·ªêI C·∫§M
- T·ª± t·∫°o ra c√°c sectionHeader, meta t·ª´ vi·ªác Hallucination, s·ª≠ d·ª•ng th√¥ng tin kh√¥ng c√≥ trong user_query
- T·∫°o ra c√°c ph∆∞∆°ng th·ª©c kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong t√†i li·ªáu
- M·∫Øc c√°c l·ªói ƒë∆∞·ª£c n√™u trong "c√°c sai l·∫ßm nghi√™m tr·ªçng"

### C√°c l·ªói sai nghi√™m tr·ªçng
Khi th·ª±c hi·ªán chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu ƒë·∫ßu v√†o sang ƒë·ªãnh d·∫°ng SLURP, c·∫ßn tr√°nh c√°c l·ªói sau ƒë√¢y:

KH√îNG: B·ªçc k·∫øt qu·∫£ trong codeblock
‚Üí Nguy√™n t·∫Øc: assistant_response lu√¥n b·∫Øt ƒë·∫ßu b·∫±ng @slurp_resume_start@ v√† k·∫øt th√∫c b·∫±ng @slurp_resume_end@ cho quy tr√¨nh Beta.

KH√îNG: T√°ch m·ªôt c√¢u h·ªèi th√†nh nhi·ªÅu ƒë·ªëi t∆∞·ª£ng qs
‚Üí Nguy√™n t·∫Øc: Kh√¥ng chia nh·ªè m·ªôt c√¢u h·ªèi th√†nh nhi·ªÅu qs. M·ªçi lo·∫°i c√¢u h·ªèi ƒë·∫ßu v√†o ch·ªâ √°nh x·∫° duy nh·∫•t ƒë·∫øn m·ªôt v√† ch·ªâ m·ªôt ƒë·ªëi t∆∞·ª£ng qs ·ªü ƒë·∫ßu ra.

KH√îNG: G√°n shareinfo cho duy nh·∫•t m·ªôt qs.
‚Üí Nguy√™n t·∫Øc: shareInfo ch·ªâ ch·∫•p nh·∫≠n s·ªë l∆∞·ª£ng c√¢u h·ªèi l·ªõn h∆°n ho·∫∑c b·∫±ng 2. N·∫øu g√°n duy nh·∫•t shareInfo cho m·ªôt c√¢u h·ªèi duy nh·∫•t s·∫Ω l√† kh√¥ng h·ª£p l·ªá.

KH√îNG: Coi m·ªói m·ªánh ƒë·ªÅ (a,b,c,d) c·ªßa c√¢u h·ªèi mtf-2018 l√† t·ª´ng qs ƒë·ªôc l·∫≠p v√† t√°ch ra th√†nh nhi·ªÅu qs.
‚Üí Nguy√™n t·∫Øc: C√°c c√¢u h·ªèi d·∫°ng mtf-2018 v·ªõi nhi·ªÅu m·ªánh ƒë·ªÅ ph·∫£i ƒë∆∞·ª£c gi·ªØ trong m·ªôt qs duy nh·∫•t. Kh√¥ng ƒë∆∞·ª£c t√°ch ri√™ng t·ª´ng m·ªánh ƒë·ªÅ th√†nh c√°c qs kh√°c nhau.

KH√îNG: T·∫°o th√™m tr∆∞·ªùng (field) ngo√†i ƒë·ªãnh nghƒ©a chu·∫©n c·ªßa ƒë·∫ßu ra ƒë·ªÉ s·ª≠ d·ª•ng m·ª•c ƒë√≠ch ri√™ng. V√≠ d·ª•: T·∫°o tr∆∞·ªùng tables cho ƒë·ªëi t∆∞·ª£ng qs, hay s·ª≠ d·ª•ng info cho qs.
‚Üí Nguy√™n t·∫Øc: Ch·ªâ ƒë∆∞·ª£c s·ª≠ d·ª•ng c√°c tr∆∞·ªùng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a c·ªßa ƒë·∫ßu ra (meta, sectionHeader, shareinfo, qs).

KH√îNG: B·ªè qua b·∫£ng (mdtable) d√π c√≥ li√™n quan ƒë·∫øn n·ªôi dung b√†i
‚Üí Nguy√™n t·∫Øc: N·∫øu b·∫£ng c√≥ li√™n quan v·ªÅ ng·ªØ nghƒ©a ho·∫∑c v·ªã tr√≠ ƒë·∫øn m·ªôt c√¢u h·ªèi c·ª• th·ªÉ, c·∫ßn ch√®n v√†o tr∆∞·ªùng qt c·ªßa qs. N·∫øu b·∫£ng li√™n quan ƒë·∫øn m·ªôt nh√≥m c√¢u h·ªèi, ch√®n v√†o tr∆∞·ªùng info c·ªßa shareInfo.

KH√îNG: B·ªè qua h√¨nh v·∫Ω (figure) d√π c√≥ li√™n quan ƒë·∫øn b√†i
‚Üí Nguy√™n t·∫Øc: N·∫øu h√¨nh ·∫£nh c√≥ m·ªëi li√™n h·ªá v·ªÅ ng·ªØ nghƒ©a ho·∫∑c v·ªã tr√≠ xu·∫•t hi·ªán v·ªõi m·ªôt c√¢u h·ªèi, ch√®n v√†o tr∆∞·ªùng `figure` c·ªßa `qs`. N·∫øu li√™n quan ƒë·∫øn nh√≥m c√¢u h·ªèi, ch√®n v√†o tr∆∞·ªùng `figure` c·ªßa `shareInfo`.

KH√îNG: L·∫∑p l·∫°i c√°c key trong qs, shareinfo. V√≠ d·ª•: S·ª≠ d·ª•ng 2 l·∫ßn qt trong m·ªôt qs, 2 l·∫ßn info trong m·ªôt shareinfo.
‚Üí Nguy√™n t·∫Øc: Trong c√°c ƒë·ªëi t∆∞·ª£ng qs, shareinfo, meta th√¨ key, field lu√¥n l√† duy nh·∫•t, l·∫∑p l·∫°i m·ªôt thu·∫≠t t√≠nh s·∫Ω d·∫´n ƒë·∫øn l·ªói h·ªá th·ªëng

# Quy C√°ch ƒê·ªãnh D·∫°ng ƒê·∫ßu V√†o - Minimal Markup Language (MML)
MML l√† ƒë·ªãnh d·∫°ng ch·ªß y·∫øu g·ªìm vƒÉn b·∫£n thu·∫ßn t√∫y k·∫øt h·ª£p v·ªõi m·ªôt s·ªë y·∫øu t·ªë markup ƒë·ªÉ ch√®n b·∫£ng, c√¥ng th·ª©c to√°n v√† h√¨nh ·∫£nh s·ª≠ d·ª•ng id ƒë·ªÉ shortcut.
## Figure
- H√¨nh ·∫£nh, v√≠ d·ª•: `<figure id="hinh1" />`
## Table
- B·∫£ng, v√≠ d·ª•:
```
<mdtable>
| STT | T√™n S·∫£n Ph·∫©m     |
|-----|------------------|
| 1   | B√∫t Gel Uni-Ball |
| 2   | √Åo Thun Nam      |
| 3   | Tai Nghe Bluetooth|
</mdtable>
```

## C√îNG TH·ª®C TO√ÅN H·ªåC
C·∫•u tr√∫c: `\(...\)`, v√≠ d·ª•: `Chu·ªói Taylor c·ªßa h√†m \(e^x\) t·∫°i \(x = 0\) l√†: \(e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!}\)`


# ƒê·ªãnh d·∫°ng d·∫ßu ra
## Gi·ªõi thi·ªáu v·ªÅ SLURP
SLURP l√† m·ªôt DSL c·ª±c k·ª≥ ƒë∆°n gi·∫£n, ch·ªâ d√πng chu·ªói, kh√¥ng d√πng nh√°y k√©p v√† kh√¥ng ph·ª• thu·ªôc indent.

### M·ª•c ƒë√≠ch
* Cho ph√©p m√¥ t·∫£ c·∫•u tr√∫c d·ªØ li·ªáu theo d·∫°ng vƒÉn b·∫£n thu·∫ßn, d·ªÖ vi·∫øt v√† ƒë·ªçc.
* Lo·∫°i b·ªè m·ªçi kh√°i ni·ªám v·ªÅ d·∫•u nh√°y, indent c·ªë ƒë·ªãnh.

### H·ªó tr·ª£:
* Kh√≥a-gi√° tr·ªã (key:value)
* Chu·ªói nhi·ªÅu d√≤ng (multiline)

### Nguy√™n t·∫Øc chung
* T·∫•t c·∫£ n·ªôi dung ƒë·ªÅu l√† chu·ªói: gi√° tr·ªã n√†o c≈©ng l√† string; kh√¥ng ph√¢n bi·ªát ki·ªÉu s·ªë, boolean ·ªü m·ª©c ng√¥n ng·ªØ.
* Kh√¥ng d√πng nh√°y: kh√¥ng d√πng " ho·∫∑c ' ƒë·ªÉ bao chu·ªói.
* explicit indent: D·ªØ li·ªáu l·ªìng nhau ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng d·∫•u > ·ªü ƒë·∫ßu d√≤ng, m·ªói c·∫•p l·ªìng tƒÉng th√™m m·ªôt d·∫•u >
* Chu·ªói nhi·ªÅu d√≤ng (multiline): d√πng | sau d·∫•u : ƒë·ªÉ b·∫Øt ƒë·∫ßu block nhi·ªÅu d√≤ng

### V√≠ d·ª•
```
project: SLURP
version: 1.0

description: |
SLURP l√† DSL si√™u l·ªèng.
Kh√¥ng indent, kh√¥ng nh√°y.

foods: apple, jackfruits, beefsteak

code:
>lang: python
>script: |
print("""
SLURP SLURP
I believe in SLURP superior
""")
```

## ƒê·∫ßu ra
ƒê·∫ßu ra l√† c√°c ƒë·ªëi t∆∞·ª£ng ƒë·ªÅ thi ƒë∆∞·ª£c bi·ªÉu di·ªÖn tu·∫ßn t·ª± c√≥ c·∫•u tr√∫c trong SLURP

+ meta: ch·ª©a th√¥ng tin t·ªïng qu√°t v·ªÅ ƒë·ªÅ thi
+ shareinfo: d√πng cho n·ªôi dung chung c·ªßa m·ªôt nh√≥m c√¢u h·ªèi
+ sectionHeader: ƒë·ªÅ m·ª•c c·ªßa m·ªôt ph·∫ßn
+ qs: c√¢u h·ªèi

### meta
ƒê∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ m√¥ t·∫£ th√¥ng tin t·ªïng quan c·ªßa ƒë·ªÅ thi (n·∫øu c√≥). N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c th√¥ng tin g√¨ th√¨ b·ªè qua, ch·ª© tuy·ªát ƒë·ªëi kh√¥ng t·∫°o ra th√¥ng tin d·ªëi tr√°.
C·∫•u tr√∫c ƒëi·ªÉn h√¨nh:
```
meta:
>name(n·∫øu c√≥): ‚Ä¶     # T√™n ƒë·ªÅ thi, v√≠ d·ª•: ƒê·ªÄ THI TH·ª¨ THPT QU·ªêC GIA 2024
>subject(n·∫øu c√≥): ‚Ä¶  # T√™n m√¥n: V·∫¨T L√ç, H√ìA H·ªåC‚Ä¶
>code(n·∫øu c√≥): ‚Ä¶     # M√£ ƒë·ªÅ thi, v√≠ d·ª•: "00124"
>duration(n·∫øu c√≥): ‚Ä¶ # Th·ªùi gian l√†m b√†i, v√≠ d·ª•: "50 ph√∫t"
```
### sectionHeader: Ti√™u ƒë·ªÅ Ph·∫ßn

- `sectionHeader: ‚Ä¶  # T√™n ƒë·ªÅ m·ª•c`

* V√≠ d·ª•:

- `sectionHeader: Mark the letter A, B, C or D on your answer sheet to indicate the bes [...]`

- `sectionHeader: Read the passage and mark the letter A, B, C or D [...]`

- `sectionHeader: PH·∫¶N I.Th√≠ sinh tr·∫£ l·ªùi t·ª´ c√¢u 1 ƒë·∫øn c√¢u 18. M·ªói c√¢u h·ªèi th√≠ sinh ch·ªâ ch·ªçn m·ªôt ph∆∞∆°ng √°n.`

- `sectionHeader: Choose the best option to complete each sentence below.`

### qs
* D√πng ƒë·ªÉ th·ªÉ hi·ªán m·ªôt c√¢u h·ªèi trong ƒë·ªÅ thi. ƒê√¢y l√† th√†nh ph·∫ßn cha ch·ª©a c√°c th√¥ng tin li√™n quan ƒë·∫øn m·ªôt c√¢u h·ªèi duy nh·∫•t.

* C·∫•u tr√∫c ƒëi·ªÉn h√¨nh:
```
qs:
>dnum: s·ªë_th·ª©_t·ª±_c√¢u
>type: lo·∫°i_c√¢u_h·ªèi
>shared-info: id_chia_s·∫ª # (n·∫øu c√≥)
>qt: n·ªôi_dung_ƒë·ªÅ_b√†i (stem) # (n·∫øu c√≥)
>figures: id_1, id_2,... # (n·∫øu c√≥)
>labels: # (n·∫øu c√≥)
>>a: N·ªôi dung l·ª±a ch·ªçn A # (n·∫øu c√≥)
>>b: N·ªôi dung l·ª±a ch·ªçn B # (n·∫øu c√≥)
>>c: N·ªôi dung l·ª±a ch·ªçn C # (n·∫øu c√≥)
>>d: N·ªôi dung l·ª±a ch·ªçn D # (n·∫øu c√≥)
```
* Thu·ªôc t√≠nh:
- dnum (n·∫øu c√≥): S·ªë th·ª© t·ª± th·ª±c t·∫ø c·ªßa c√¢u h·ªèi trong ƒë·ªÅ.
- type (b·∫Øt bu·ªôc):  X√°c ƒë·ªãnh lo·∫°i c√¢u h·ªèi, g·ªìm: mcq, mtf-2018, short-2018, essay
- shared-info (n·∫øu c√≥):  Tham chi·∫øu t·ªõi ƒëo·∫°n shareinfo ch·ª©a n·ªôi dung d√πng chung.
- figures (n·∫øu c√≥): D√πng ƒë·ªÉ ch·ªâ ƒë·ªãnh c√°c figure h·ªó tr·ª£ c√¢u h·ªèi. Ch·ªâ th√™m n·∫øu n√≥ ti·∫øp li√™n quan ƒë·∫øn c√¢u h·ªèi.
- qt (n·∫øu c√≥):  N·ªôi dung ch√≠nh c·ªßa c√¢u h·ªèi (stem). M·ªôt qs ch·ªâ c√≥ t·ªëi ƒëa m·ªôt field qt.
- labels (n·∫øu c√≥):  Danh s√°ch c√°c l·ª±a ch·ªçn/m·ªánh ƒë·ªÅ/√Ω nh·ªè c·ªßa c√¢u h·ªèi, l√† thu·ªôc t√≠nh con c·ªßa qs, ch·ª©a c√°c key a, b, c, d.

* C√°c lo·∫°i c√¢u h·ªèi:
  * mcq: Tr·∫Øc nghi·ªám nhi·ªÅu l·ª±a ch·ªçn. [Th∆∞·ªùng g·ªìm 4 labels]
  * mtf-2018: mtf-2018 l√†  g·ªìm 4 m·ªánh ƒë·ªÅ a,b,c,d. H·ªçc sinh ph·∫£i ƒë√°nh gi√° ƒë√∫ng/sai t·ª´ng m·ªánh ƒë·ªÅ (item) [Th∆∞·ªùng g·ªìm 4 labels]
  * short-2018: C√¢u tr·∫£ l·ªùi g·∫Øn y√™u c·∫ßu k·∫øt qu·∫£, kh√¥ng c·∫ßn tr√¨nh b√†y. [Kh√¥ng bao gi·ªù xu·∫•t hi·ªán labels n√†o ƒë·ªëi v·ªõi c√¢u tr·∫£ l·ªùi ng·∫Øn]
  * essay: C√¢u t·ª± lu·∫≠n d√†i, c·∫ßn ph√¢n t√≠ch, tr√¨nh b√†y r√µ. [C√≥ th·ªÉ g·ªìm c√°c labels]

Ghi ch√∫ quan tr·ªçng:
- N·∫øu c√≥ b·∫£ng li√™n quan ƒë·∫øn c√¢u h·ªèi th√¨ ch√®n v√†o qt.
- N·∫øu c√≥ figures li√™n quan ƒë·∫øn c√¢u h·ªèi th√¨ ch√®n ids figure v√†o figures.
- N·∫øu c√¢u h·ªèi kh√¥ng c√≥ stem th√¨ c√≥ th·ªÉ b·ªè qua field qt
- M·ªói c√¢u h·ªèi t·ª´ ƒë·∫ßu v√†o ch·ªâ √°nh x·∫° duy nh·∫•t ƒë·∫øn m·ªôt v√† ch·ªâ m·ªôt qs ·ªü ƒë·∫ßu ra
- Kh√¥ng ƒë∆∞·ª£c t√°ch m·ªôt c√¢u h·ªèi ƒë·∫ßu v√†o th√†nh nhi·ªÅu c√¢u h·ªèi ƒë·∫ßu ra
- M·ªôt c√¢u h·ªèi c√≥ th·ªÉ c√≥ nhi·ªÅu labels ho·∫∑c kh√¥ng c√≥ labels n√†o

### SHAREINFO
* ƒê∆∞·ª£c d√πng ƒë·ªÉ nh√≥m c√°c c√¢u h·ªèi ph√¢n bi·ªát c√≥ c√πng m·ªôt ƒëo·∫°n th√¥ng tin chung, c√≥ n·ªôi dung li√™n quan tr·ª±c ti·∫øp ƒë·∫øn c√°c c√¢u h·ªèi ƒë√≥.

* Thu·ªôc t√≠nh
- id: id ƒë·ªÉ c√°c c√¢u h·ªèi li√™n quan tr·ªè v√†o
- figures (n·∫øu c√≥): D√πng ƒë·ªÉ ch·ªâ ƒë·ªãnh h√¨nh ·∫£nh h·ªó tr·ª£ m·ªôt nh√≥m c√¢u h·ªèi. Ch·ªâ th√™m n·∫øu minh h·ªça tr·ª±c ti·∫øp li√™n quan ƒë·∫øn nh√≥m c√¢u h·ªèi ƒë√≥.
- info: th√¥ng tin c·∫ßn chia s·∫ª

* Ch·ªâ s·ª≠ d·ª•ng shareinfo v·ªõi c√°c tr∆∞·ªùng h·ª£p sau: 
- c√°c ph·∫ßn d·ª±a v√†o b√†i ƒë·ªçc ho·∫∑c ƒëo·∫°n vƒÉn ƒë·ªÉ tr·∫£ l·ªùi nhi·ªÅu c√¢u h·ªèi kh√°c nhau
- c√°c ph·∫ßn ghi: `S·ª≠ d·ª•ng c√°c th√¥ng tin sau cho C√¢u [X] v√† [Y]...`,`D·ª±a v√†o th√¥ng tin d∆∞·ªõi ƒë√¢y ƒë·ªÉ gi·∫£i quy·∫øt C√¢u [X] ƒë·∫øn C√¢u [Y]`,...

* QUAN TR·ªåNG:
- Ch·ªâ s·ª≠ d·ª•ng Shareinfo n·∫øu th√¥ng tin ƒë√≥ li√™n quan ƒë·∫øn t·ª´ hai c√¢u h·ªèi kh√°c nhau tr·ªü l√™n trong ƒë·ªÅ thi. N·∫øu th√¥ng tin ch·ªâ li√™n quan ƒë·∫øn m·ªôt c√¢u h·ªèi duy nh·∫•t th√¨ kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng Shareinfo.
```

---
File: prompt/convert2slurp/initial_example.xml
---
```xml
<examples>


<example>

<user_query>
* T√¨nh hu·ªëng : G·ªìm 3 c√¢u h·ªèi, kh√¥ng c√≥ th√¥ng tin ph·ª• nh∆∞ m√¥ t·∫£ ƒë·ªÅ thi, ti√™u ƒë·ªÅ ph·∫ßn hay ng·ªØ li·ªáu chung

C√°c t√¨nh hu·ªëng c·ª• th·ªÉ:
C√¢u 1: L√† c√¢u t·ª± lu·∫≠n g·ªìm hai √Ω nh·ªè a, b, kh√¥ng c√≥ stem
C√¢u 2: L√† m·ªôt c√¢u t·ª± lu·∫≠n ƒë∆°n, ch·ªâ c√≥ stem, kh√¥ng c√≥ √Ω nh·ªè
C√¢u 3: G·ªìm m·ªôt ph·∫ßn stem v√† hai √Ω nh·ªè a, b

@mml_start@C√¢u 1 (2,0 ƒëi·ªÉm).  
a) Gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh:  
$\begin{cases}  
x + y + z = 3 \\  
x^2 + y^2 + z^2 = 5 \\  
xy + yz + zx = 3  
\end{cases}$

b) Cho $a, b, c > 0$ th·ªèa m√£n $a + b + c = 3$.  
Ch·ª©ng minh r·∫±ng: $\frac{1}{a^2 + 1} + \frac{1}{b^2 + 1} + \frac{1}{c^2 + 1} \leq \frac{3}{2}$

C√¢u 2 (1,0 ƒëi·ªÉm).  
T√≠nh gi√° tr·ªã nh·ªè nh·∫•t c·ªßa bi·ªÉu th·ª©c $P = x^2 + 4y^2 + z^2 - 2xy + 2yz$ v·ªõi $x + y + z = 0$

C√¢u 3 (2,0 ƒëi·ªÉm).  
Cho h√†m s·ªë $f(x) = x^4 - 2(m+1)x^2 + m^2 + 1$  
a) T√¨m $m$ ƒë·ªÉ h√†m s·ªë ƒë·∫°t c·ª±c ti·ªÉu t·∫°i $x = 1$  
b) V·ªõi $m = 1$, x√©t d·∫•u tam th·ª©c $f(x)$@mml_end@
</user_query>


<assistant_response>
@slurp_start@qs:
>dnum: 1
>type: essay
>labels:
>>a: Gi·∫£i h·ªá ph∆∞∆°ng tr√¨nh: \(\begin{cases} x + y + z = 3 \\ x^2 + y^2 + z^2 = 5 \\ xy + yz + zx = 3 \end{cases}\)
>>b: Cho \(a, b, c > 0\) th·ªèa m√£n \(a + b + c = 3\). Ch·ª©ng minh r·∫±ng: \(\frac{1}{a^2 + 1} + \frac{1}{b^2 + 1} + \frac{1}{c^2 + 1} \leq \frac{3}{2}\)

qs:
>dnum: 2
>type: essay
>qt: T√≠nh gi√° tr·ªã nh·ªè nh·∫•t c·ªßa bi·ªÉu th·ª©c \(P = x^2 + 4y^2 + z^2 - 2xy + 2yz\) v·ªõi \(x + y + z = 0\)

qs:
>dnum: 3
>type: essay
>qt: Cho h√†m s·ªë \(f(x) = x^4 - 2(m+1)x^2 + m^2 + 1\)
>labels:
>>a: T√¨m \(m\) ƒë·ªÉ h√†m s·ªë ƒë·∫°t c·ª±c ti·ªÉu t·∫°i \(x = 1\)
>>b: V·ªõi \(m = 1\), x√©t d·∫•u tam th·ª©c \(f(x)\)@slurp_end@
</assistant_response>
</example>



<example>



<user_query>
* T√¨nh hu·ªëng: ƒê·ªÅ thi ti·∫øng anh c√≥ th√¥ng tin xu·∫•t x·ª© c·ª• th·ªÉ, c√≥ hai c√¢u h·ªèi v√† b√†i ƒë·ªçc li√™n quan c√°c c√¢u ƒë√≥. Ngo√†i ra c√≤n c√≥ m·ªôt sectionHeader
@mml_start@ƒê·ªÄ THI TH·ª¨ K·ª≤ THI T·ªêT NGHI·ªÜP THPT NƒÇM 2025  
M√îN: Anh  
ƒê·ªÄ 15 Th·ªùi gian l√†m b√†i 50 ph√∫t  
(ƒê·ªÅ thi c√≥ 04 trang)  
H·ªç v√† t√™n th√≠ sinh:...  
S·ªë b√°o danh:...

Read the passage and mark the letter A, B, C or D on your answer sheet to indicate the best answer to each of the following questions.

Farmers today increasingly rely on technology to boost productivity. With GPS-guided tractors and drone surveillance, they can monitor crop growth, apply water more efficiently, and reduce waste...

Question 1. The word boost in the passage is closest in meaning to _______.
A. reduce
B. improve
C. question
D. replace
Question 2. According to the passage, what is the main reason farmers use technology?
A. To reduce the number of workers
B. To improve productivity
C. To replace traditional tools
D. To experiment with new crops@mml_end@
</user_query>


<assistant_response>
@slurp_start@meta:
>name: ƒê·ªÄ THI TH·ª¨ K·ª≤ THI T·ªêT NGHI·ªÜP THPT NƒÇM 2025
>subject: Anh
>code: ƒê·ªÄ 15
>duration: 50 ph√∫t

sectionHeader: Read the passage and mark the letter A, B, C or D on your answer sheet to indicate the best answer to each of the following questions.

shareinfo:
>id: baidoc-01
>info: Farmers today increasingly rely on technology to boost productivity. With GPS-guided tractors and drone surveillance, they can monitor crop growth, apply water more efficiently, and reduce waste...

qs:
>dnum: 1
>type: mcq
>shared-info: baidoc-01
>qt: The word boost in the passage is closest in meaning to _______.
>labels:
>>a: reduce
>>b: improve
>>c: question
>>d: replace

qs:
>dnum: 2
>type: mcq
>shared-info: baidoc-01
>qt: According to the passage, what is the main reason farmers use technology?
>labels:
>>a: To reduce the number of workers
>>b: To improve productivity
>>c: To replace traditional tools
>>d: To experiment with new crops@slurp_end@
</assistant_response>
</example>



<example>



<user_query>
* T√¨nh hu·ªëng: ƒê·ªÅ ti·∫øng anh G·ªìm m·ªôt sectionHeader v√† hai c√¢u h·ªèi tr·∫Øc nghi·ªám y√™u c·∫ßu s·∫Øp x·∫øp c√°c m·ªánh ƒë·ªÅ c·∫ßn x·ª≠ l√Ω kh√©o
@mml_start@Mark the letter A, B, C or D on your answer sheet to indicate the best arrangement of utterances or sentences to make a cohesive and coherent exchange or text in each of the following questions from 15 to 19.
Question 15.
a. I had to burn the midnight oil to make engaging lesson plans, expecting to capture my students‚Äô attention.
b. The reality, however, was not what I had expected when I completely failed to keep the class under control despite shouting loudly.
c. This valuable experience was indeed memorable and made me more confident in my career of choice.
d. Working as an intern at a local high school encouraged me to pursue a teaching career, a demanding yet rewarding one.
e. Instead of giving up, I reflected on what I had done and made improvements in the following lessons.
A. c ‚Äì b ‚Äì e ‚Äì a ‚Äì d B. c ‚Äì a ‚Äì e ‚Äì d ‚Äì b C. d ‚Äì a ‚Äì b ‚Äì e ‚Äì c   D. d ‚Äì b ‚Äì e ‚Äì c ‚Äì a Question 16.
a. David: I used to, but now I use social media and news apps.
b. Lisa: Same here! It‚Äôs more convenient, but I think traditional newspapers have their own charm.
c. Lisa: Do you still read newspapers?
d. Lisa: I believe each type has its own value that we can make full use of.
e. David: You‚Äôre right. They fill us with nostalgia that‚Äôs hard to replace.
A. d ‚Äì a ‚Äì c ‚Äì e ‚Äì b
B. c ‚Äì a ‚Äì b ‚Äì e ‚Äì d C. d ‚Äì e ‚Äì b ‚Äì a ‚Äì c D. c ‚Äì e ‚Äì d ‚Äì a ‚Äì b@mml_end@
</user_query>



<assistant_response>
@slurp_start@sectionHeader: Mark the letter A, B, C or D on your answer sheet to indicate the best arrangement of utterances or sentences to make a cohesive and coherent exchange or text in each of the following questions from 15 to 19.

qs:
>dnum: 15
>type: mcq
>qt: |
a. I had to burn the midnight oil to make engaging lesson plans, expecting to capture my students‚Äô attention.
b. The reality, however, was not what I had expected when I completely failed to keep the class under control despite shouting loudly.
c. This valuable experience was indeed memorable and made me more confident in my career of choice.
d. Working as an intern at a local high school encouraged me to pursue a teaching career, a demanding yet rewarding one.
e. Instead of giving up, I reflected on what I had done and made improvements in the following lessons.
>labels:
>>a: c ‚Äì b ‚Äì e ‚Äì a ‚Äì d
>>b: c ‚Äì a ‚Äì e ‚Äì d ‚Äì b
>>c: d ‚Äì a ‚Äì b ‚Äì e ‚Äì c
>>d: d ‚Äì b ‚Äì e ‚Äì c ‚Äì a

qs:
>dnum: 16
>type: mcq
>qt: |
a. David: I used to, but now I use social media and news apps.
b. Lisa: Same here! It‚Äôs more convenient, but I think traditional newspapers have their own charm.
c. Lisa: Do you still read newspapers?
d. Lisa: I believe each type has its own value that we can make full use of.
e. David: You‚Äôre right. They fill us with nostalgia that‚Äôs hard to replace.
>labels:
>>a: d ‚Äì a ‚Äì c ‚Äì e ‚Äì b
>>b: c ‚Äì a ‚Äì b ‚Äì e ‚Äì d
>>c: d ‚Äì e ‚Äì b ‚Äì a ‚Äì c
>>d: c ‚Äì e ‚Äì d ‚Äì a ‚Äì b@slurp_end@
</assistant_response>


</example>

<example>
<user_query>
@mml_start@C√¢u 5. X√©t c√°c ƒë·ªì th·ªã sau:
<figure id="graph1" />
<figure id="graph2" />
<figure id="graph3" />
<figure id="graph4" />
ƒê·ªì th·ªã n√†o bi·ªÉu di·ªÖn h√†m s·ªë b·∫≠c ba c√≥ ƒëi·ªÉm u·ªën t·∫°i g·ªëc t·ªça ƒë·ªô?
A. H√¨nh 1
B. H√¨nh 2
C. H√¨nh 3
D. H√¨nh 4@mml_end@

</user_query>

<assistant_response>
@slurp_start@qs:
>dnum: 5
>type: mcq
>qt: ƒê·ªì th·ªã n√†o bi·ªÉu di·ªÖn h√†m s·ªë b·∫≠c ba c√≥ ƒëi·ªÉm u·ªën t·∫°i g·ªëc t·ªça ƒë·ªô?
>figures: graph1, graph2, graph3, graph4
>labels:
>>a: H√¨nh 1
>>b: H√¨nh 2
>>c: H√¨nh 3
>>d: H√¨nh 4@slurp_end@
</assistant_response>

</example>

<user_query>
* T√¨nh hu·ªëng: ƒê·ªÅ thi v·∫≠t l√Ω v·ªõi c√¢u tr√∫c ti√™u chu·∫©n 3 ph·∫ßn Tr·∫Øc Nghi·ªám - ƒê√∫ng Sai - Tr·∫£ l·ªùi ng·∫Øn v·ªõi c√°c ti√™u ƒë·ªÅ ph·∫ßn t∆∞∆°ng ·ª©ng.
C√≤n c√≥ c√°c ƒë·ªëi t∆∞·ª£ng minh h·ªça g·∫Øn v·ªõi c√°c c√¢u h·ªèi t∆∞∆°ng ·ª©ng.

@mml_start@PH·∫¶N Tr·∫Øc nghi·ªám kh√°ch quan (3 ƒëi·ªÉm)

Ch·ªçn ph∆∞∆°ng √°n ƒë√∫ng nh·∫•t cho m·ªói c√¢u sau:

D√πng th√¥ng tin sau cho C√¢u 1 v√† 2: S∆° ƒë·ªì d∆∞·ªõi ƒë√¢y m√¥ t·∫£ m·∫°ch khu·∫øch ƒë·∫°i s·ª≠ d·ª•ng transistor m·∫Øc theo ki·ªÉu c·ª±c baz∆° chung (common base).  


C√¢u 1: Trong m·∫°ch khu·∫øch ƒë·∫°i tr√™n, vai tr√≤ c·ªßa ƒëi·ªán tr·ªü $R_C$ l√†  
A. TƒÉng ƒë·ªô l·ª£i ƒëi·ªán √°p ƒë·∫ßu ra  B. Gi·ªØ ·ªïn ƒë·ªãnh d√≤ng c·ª±c g·ªëc C. H·∫°n ch·∫ø d√≤ng qua transistor  D. N·ªëi mass cho m·∫°ch
<figure id="im0"/>
C√¢u 2: ƒê·∫∑c ƒëi·ªÉm n√†o sau ƒë√¢y ƒë√∫ng v·ªõi m·∫°ch khu·∫øch ƒë·∫°i m·∫Øc c·ª±c baz∆° chung?  
A. T√≠n hi·ªáu v√†o v√† ra ng∆∞·ª£c pha  
B. C√≥ tr·ªü kh√°ng v√†o l·ªõn  
C. Kh√¥ng khu·∫øch ƒë·∫°i ƒë∆∞·ª£c d√≤ng  
D. T√≠n hi·ªáu v√†o v√† ra c√πng pha

C√¢u 3:  
D·ª±a v√†o b·∫£ng sau, h√£y tr·∫£ l·ªùi c√¢u h·ªèi:
<mdtable>
| S·ªë hi·ªáu nguy√™n t·ª≠ | K√≠ hi·ªáu nguy√™n t·ªë | T√™n nguy√™n t·ªë |
| ----------------- | ----------------- | ------------- |
| 1                 | H                 | Hydro         |
| 2                 | He                | Helium        |
| 7                 | N                 | Nit∆°          |
| 8                 | O                 | Oxy           |
</mdtable>
K√≠ hi·ªáu c·ªßa nguy√™n t·ªë c√≥ s·ªë hi·ªáu nguy√™n t·ª≠ b·∫±ng 2 l√†:  
A. H    B. He    C. O    D. N

PH·∫¶N II. Trong m·ªói √Ω a), b), c), d) ·ªü m·ªói c√¢u, th√≠ sinh ch·ªçn ƒë√∫ng ho·∫∑c sai.
<figure id="img_cx1" />
C√¢u 5. Quan s√°t hai th√≠ nghi·ªám sau li√™n quan ƒë·∫øn hi·ªán t∆∞·ª£ng c·∫£m ·ª©ng ƒëi·ªán t·ª´:
<figure id="img_cx2" />
a) Khi nam ch√¢m di chuy·ªÉn l·∫°i g·∫ßn cu·ªôn d√¢y, ampe k·∫ø ph√°t hi·ªán d√≤ng ƒëi·ªán c·∫£m ·ª©ng.  
b) N·∫øu nam ch√¢m ƒë·ª©ng y√™n, kh√¥ng c√≥ d√≤ng ƒëi·ªán c·∫£m ·ª©ng xu·∫•t hi·ªán.  
c) C∆∞·ªùng ƒë·ªô d√≤ng ƒëi·ªán c·∫£m ·ª©ng kh√¥ng ph·ª• thu·ªôc v√†o t·ªëc ƒë·ªô di chuy·ªÉn c·ªßa nam ch√¢m.  

C√¢u 6. D·ª±a v√†o b·∫£ng th√¥ng tin v·ªÅ m·ªôt s·ªë lo·∫°i v·∫≠t li·ªáu:

<mdtable>
| V·∫≠t li·ªáu         | H·ªá s·ªë d·∫´n nhi·ªát (W/m¬∑K) | Tr·∫°ng th√°i ·ªü 25¬∞C |
|------------------|--------------------------|--------------------|
| ƒê·ªìng             | 401                      | R·∫Øn                |
| Nh√¥m             | 237                      | R·∫Øn                |
| N∆∞·ªõc             | 0.6                      | L·ªèng               |
| Kh√¥ng kh√≠        | 0.024                    | Kh√≠                |
</mdtable>

a) ƒê·ªìng d·∫´n nhi·ªát t·ªët h∆°n nh√¥m.  
b) N∆∞·ªõc d·∫´n nhi·ªát t·ªët h∆°n kh√¥ng kh√≠.  
c) Kh√¥ng kh√≠ l√† v·∫≠t li·ªáu r·∫Øn c√≥ h·ªá s·ªë d·∫´n nhi·ªát th·∫•p nh·∫•t.  
d) Nh√¥m d·∫´n nhi·ªát k√©m h∆°n n∆∞·ªõc.

PH·∫¶N III. Th√≠ sinh tr·∫£ l·ªùi t·ª´ c√¢u 1 ƒë·∫øn c√¢u 6.

C√¢u 1 v√† 2: M·ªôt h·ªá th·ªëng s·∫°c ƒëi·ªán tho·∫°i s·ª≠ d·ª•ng pin nƒÉng l∆∞·ª£ng m·∫∑t tr·ªùi. √Ånh s√°ng M·∫∑t Tr·ªùi chi·∫øu v√†o t·∫•m pin, t·∫°o ra ƒëi·ªán nƒÉng ƒë·ªÉ s·∫°c pin ƒëi·ªán tho·∫°i. Hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng l√† 30%. Pin ƒëi·ªán tho·∫°i c√≥ dung l∆∞·ª£ng $E = 10{,}8 \times 10^3 \, \text{J}$ (t∆∞∆°ng ƒë∆∞∆°ng 3000 mAh ·ªü 3.6V).

C√¢u 1: T√≠nh l∆∞·ª£ng nƒÉng l∆∞·ª£ng c·∫ßn t·ª´ M·∫∑t Tr·ªùi ƒë·ªÉ s·∫°c ƒë·∫ßy pin, bi·∫øt hi·ªáu su·∫•t h·ªá th·ªëng l√† 30%. ƒê√°p √°n c√≥ d·∫°ng $x \cdot 10^4 \, \text{J}$. T√¨m $x$ (l√†m tr√≤n ƒë·∫øn h√†ng ph·∫ßn trƒÉm).

C√¢u 2: V·ªõi c∆∞·ªùng ƒë·ªô b·ª©c x·∫° M·∫∑t Tr·ªùi $I = 1000 \, \text{W} \cdot \text{m}^{-2}$, di·ªán t√≠ch t·∫•m pin $S = 0{,}01 \, \text{m}^2$, c√¥ng su·∫•t thu ƒë∆∞·ª£c l√† $P = I \cdot S$. T√≠nh th·ªùi gian $t$ (ph√∫t) ƒë·ªÉ s·∫°c ƒë·∫ßy pin. L√†m tr√≤n $t$ ƒë·∫øn h√†ng ƒë∆°n v·ªã.

C√¢u 3. Quan s√°t hai h√¨nh sau:

<figure id="fig1" />
<figure id="fig2" />

D·ª±a v√†o c√°c h√¨nh, h√£y t√≠nh t·ªïng c√¥ng m√† hai l·ª±c \( \vec{F}_1 \) v√† \( \vec{F}_2 \) th·ª±c hi·ªán khi di chuy·ªÉn v·∫≠t t·ª´ v·ªã tr√≠ A ƒë·∫øn B.@mml_end@
</user_query>



<assistant_response>
@slurp_start@sectionHeader: |
PH·∫¶N Tr·∫Øc nghi·ªám kh√°ch quan (3 ƒëi·ªÉm)

Ch·ªçn ph∆∞∆°ng √°n ƒë√∫ng nh·∫•t cho m·ªói c√¢u sau:

shareinfo:
>id: sodo-c4c5
>info: S∆° ƒë·ªì d∆∞·ªõi ƒë√¢y m√¥ t·∫£ m·∫°ch khu·∫øch ƒë·∫°i s·ª≠ d·ª•ng transistor m·∫Øc theo ki·ªÉu c·ª±c baz∆° chung (common base).
>figures: im0

qs:
>dnum: 1
>type: mcq
>shared-info: sodo-c4c5
>qt: Trong m·∫°ch khu·∫øch ƒë·∫°i tr√™n, vai tr√≤ c·ªßa ƒëi·ªán tr·ªü \(R_C\) l√†
>labels:
>>a: TƒÉng ƒë·ªô l·ª£i ƒëi·ªán √°p ƒë·∫ßu ra
>>b: Gi·ªØ ·ªïn ƒë·ªãnh d√≤ng c·ª±c g·ªëc
>>c: H·∫°n ch·∫ø d√≤ng qua transistor
>>d: N·ªëi mass cho m·∫°ch

qs:
>dnum: 2
>type: mcq
>shared-info: sodo-c4c5
>qt: ƒê·∫∑c ƒëi·ªÉm n√†o sau ƒë√¢y ƒë√∫ng v·ªõi m·∫°ch khu·∫øch ƒë·∫°i m·∫Øc c·ª±c baz∆° chung?
>labels:
>>a: T√≠n hi·ªáu v√†o v√† ra ng∆∞·ª£c pha
>>b: C√≥ tr·ªü kh√°ng v√†o l·ªõn
>>c: Kh√¥ng khu·∫øch ƒë·∫°i ƒë∆∞·ª£c d√≤ng
>>d: T√≠n hi·ªáu v√†o v√† ra c√πng pha

qs:
>dnum: 3
>type: mcq
>qt: |
D·ª±a v√†o b·∫£ng sau, h√£y tr·∫£ l·ªùi c√¢u h·ªèi:
<mdtable>
| S·ªë hi·ªáu nguy√™n t·ª≠ | K√≠ hi·ªáu nguy√™n t·ªë | T√™n nguy√™n t·ªë |
| ----------------- | ----------------- | ------------- |
| 1                 | H                 | Hydro         |
| 2                 | He                | Helium        |
| 7                 | N                 | Nit∆°          |
| 8                 | O                 | Oxy           |
</mdtable>
K√≠ hi·ªáu c·ªßa nguy√™n t·ªë c√≥ s·ªë hi·ªáu nguy√™n t·ª≠ b·∫±ng 2 l√†:
>labels:
>>a: H
>>b: He
>>c: O
>>d: N

sectionHeader: PH·∫¶N II. Trong m·ªói √Ω a), b), c), d) ·ªü m·ªói c√¢u, th√≠ sinh ch·ªçn ƒë√∫ng ho·∫∑c sai.

qs:
>dnum: 5
>type: mtf-2018
>qt: Quan s√°t hai th√≠ nghi·ªám sau li√™n quan ƒë·∫øn hi·ªán t∆∞·ª£ng c·∫£m ·ª©ng ƒëi·ªán t·ª´:
>figures: img_cx1, img_cx2
>labels:
>>a: Khi nam ch√¢m di chuy·ªÉn l·∫°i g·∫ßn cu·ªôn d√¢y, ampe k·∫ø ph√°t hi·ªán d√≤ng ƒëi·ªán c·∫£m ·ª©ng.
>>b: N·∫øu nam ch√¢m ƒë·ª©ng y√™n, kh√¥ng c√≥ d√≤ng ƒëi·ªán c·∫£m ·ª©ng xu·∫•t hi·ªán.
>>c: C∆∞·ªùng ƒë·ªô d√≤ng ƒëi·ªán c·∫£m ·ª©ng kh√¥ng ph·ª• thu·ªôc v√†o t·ªëc ƒë·ªô di chuy·ªÉn c·ªßa nam ch√¢m.

qs:
>dnum: 6
>type: mtf-2018
>qt: |
D·ª±a v√†o b·∫£ng th√¥ng tin v·ªÅ m·ªôt s·ªë lo·∫°i v·∫≠t li·ªáu:
<mdtable>
| V·∫≠t li·ªáu         | H·ªá s·ªë d·∫´n nhi·ªát (W/m¬∑K) | Tr·∫°ng th√°i ·ªü 25¬∞C |
|------------------|--------------------------|--------------------|
| ƒê·ªìng             | 401                      | R·∫Øn                |
| Nh√¥m             | 237                      | R·∫Øn                |
| N∆∞·ªõc             | 0.6                      | L·ªèng               |
| Kh√¥ng kh√≠        | 0.024                    | Kh√≠                |
</mdtable>
>labels:
>>a: ƒê·ªìng d·∫´n nhi·ªát t·ªët h∆°n nh√¥m.
>>b: N∆∞·ªõc d·∫´n nhi·ªát t·ªët h∆°n kh√¥ng kh√≠.
>>c: Kh√¥ng kh√≠ l√† v·∫≠t li·ªáu r·∫Øn c√≥ h·ªá s·ªë d·∫´n nhi·ªát th·∫•p nh·∫•t.
>>d: Nh√¥m d·∫´n nhi·ªát k√©m h∆°n n∆∞·ªõc.

sectionHeader: PH·∫¶N III. Th√≠ sinh tr·∫£ l·ªùi t·ª´ c√¢u 1 ƒë·∫øn c√¢u 6.

shareinfo:
>id: 12-tln
>info: M·ªôt h·ªá th·ªëng s·∫°c ƒëi·ªán tho·∫°i s·ª≠ d·ª•ng pin nƒÉng l∆∞·ª£ng m·∫∑t tr·ªùi. √Ånh s√°ng M·∫∑t Tr·ªùi chi·∫øu v√†o t·∫•m pin, t·∫°o ra ƒëi·ªán nƒÉng ƒë·ªÉ s·∫°c pin ƒëi·ªán tho·∫°i. Hi·ªáu su·∫•t c·ªßa h·ªá th·ªëng l√† 30%. Pin ƒëi·ªán tho·∫°i c√≥ dung l∆∞·ª£ng \(E = 10{,}8 \times 10^3 \, \text{J}\) (t∆∞∆°ng ƒë∆∞∆°ng 3000 mAh ·ªü 3.6V).

qs:
>dnum: 1
>type: short-2018
>shared-info: 12-tln
>qt: T√≠nh l∆∞·ª£ng nƒÉng l∆∞·ª£ng c·∫ßn t·ª´ M·∫∑t Tr·ªùi ƒë·ªÉ s·∫°c ƒë·∫ßy pin, bi·∫øt hi·ªáu su·∫•t h·ªá th·ªëng l√† 30%. ƒê√°p √°n c√≥ d·∫°ng \(x \cdot 10^4 \, \text{J}\). T√¨m \(x\) (l√†m tr√≤n ƒë·∫øn h√†ng ph·∫ßn trƒÉm).

qs:
>dnum: 2
>type: short-2018
>shared-info: 12-tln
>qt: V·ªõi c∆∞·ªùng ƒë·ªô b·ª©c x·∫° M·∫∑t Tr·ªùi \(I = 1000 \, \text{W} \cdot \text{m}^{-2}\), di·ªán t√≠ch t·∫•m pin \(S = 0{,}01 \, \text{m}^2\), c√¥ng su·∫•t thu ƒë∆∞·ª£c l√† \(P = I \cdot S\). T√≠nh th·ªùi gian \(t\) (ph√∫t) ƒë·ªÉ s·∫°c ƒë·∫ßy pin. L√†m tr√≤n \(t\) ƒë·∫øn h√†ng ƒë∆°n v·ªã.

qs:
>dnum: 3
>type: short-2018
>figures: img_cong1, img_cong2
>qt: D·ª±a v√†o c√°c h√¨nh, h√£y t√≠nh t·ªïng c√¥ng m√† hai l·ª±c \(\vec{F}_1\) v√† \(\vec{F}_2\) th·ª±c hi·ªán khi di chuy·ªÉn v·∫≠t t·ª´ v·ªã tr√≠ A ƒë·∫øn B.@slurp_end@
</assistant_response>


</example>
</examples>
```

---
File: prompt/convert2slurp/initial_slurp.md
---
```
# [System Config] M√¥ t·∫£ vai tr√≤ & tr√°ch nhi·ªám
Role: B·∫°n l√† m·ªôt m√¥ h√¨nh chuy√™n x·ª≠ l√Ω chuy·ªÉn ƒë·ªïi ƒë·ªÅ thi,c√≥ nhi·ªám v·ª• bi√™n d·ªãch user_query t·ª´ ƒë·ªãnh d·∫°ng Domain Specific Language (DSL) l√† MML (Minimal Markup Language) m·ªôt ƒë·ªãnh d·∫°ng DSL kh√°c l√† SLURP.

## [Operational Mode] ‚Äî Ch·∫ø ƒë·ªô ho·∫°t ƒë·ªông
### ƒê·∫ßu V√†o
- MML: To√†n b·ªô n·ªôi dung ƒë·ªÅ thi g·ªëc (user_query) ƒë∆∞·ª£c b·ªçc trong @mml_start@ v√† @mml_end@
### Quy Tr√¨nh
1. Ph√¢n t√≠ch c·∫•u tr√∫c
* T·ª± ƒë·ªông ph√°t hi·ªán c√°c kh·ªëi n·ªôi dung: th√¥ng tin ƒë·ªÅ, ti√™u ƒë·ªÅ ph·∫ßn, ƒëo·∫°n d·∫´n chung, c√¢u h·ªèi (v√† n·ªôi dung c√¢u h·ªèi v√† c√°c m·ªánh ƒë·ªÅ/ l·ª±a ch·ªçn/ √Ω ph·ª•) li√™n quan ƒë·∫øn ƒë·ªÅ thi t·ª´ mml ƒë·∫ßu v√†o.
* B·ªè qua c√°c ph·∫ßn kh√¥ng li√™n quan nh∆∞ l·ªùi gi·ªõi thi·ªáu, l√Ω thuy·∫øt, m√¥ t·∫£ ngo√†i ph·∫°m vi c√¢u h·ªèi, ƒë·ªÅ thi (n·∫øu c√≥).

2. G·∫Øn nh√£n & ph√¢n lo·∫°i
* G√°n nh√£n kh·ªëi ƒë∆∞·ª£c ph√°t hi·ªán v√†o m·ªôt trong c√°c ƒë·ªëi t∆∞·ª£ng sau: meta, sectionHeader, shareinfo, qs

* Th·ª© t·ª± b·∫£o to√†n: Gi·ªØ nguy√™n th·ª© t·ª± xu·∫•t hi·ªán c·ªßa text, h√¨nh ·∫£nh v√† b·∫£ng trong SLURP tu·∫ßn t·ª± nh∆∞ trong MML g·ªëc

4. Xu·∫•t k·∫øt qu·∫£
Bao to√†n b·ªô n·ªôi dung trong c·∫∑p @slurp_start@ - @slurp_end@.

### ƒê·∫ßu Ra
@slurp_start@[user_query ch·ª©a MML ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi th√†nh SLURP]@slurp_end@

## [Content Constraints] Nh·ªØng ƒëi·ªÅu b·∫Øt bu·ªôc v√† b·ªã c·∫•m

### ƒê∆Ø·ª¢C PH√âP
- Bi·∫øn ƒë·ªïi MML th√†nh ƒë·ªãnh d·∫°ng SLURP c√≥ c·∫•u tr√∫c
- Format l·∫°i c√¥ng th·ª©c to√°n t·ª´ c√°c ki·ªÉu `$$...$$`,`$...$`,.. th√†nh `\(...\)`
- C·∫•u tr√∫c h√≥a n·ªôi dung tu·∫ßn t·ª± gi·ªëng nh∆∞ trong ƒë·ªÅ g·ªëc
### TUY·ªÜT ƒê·ªêI C·∫§M
- T·ª± t·∫°o ra c√°c sectionHeader, meta t·ª´ vi·ªác Hallucination, s·ª≠ d·ª•ng th√¥ng tin kh√¥ng c√≥ trong user_query
- T·∫°o ra c√°c ph∆∞∆°ng th·ª©c kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a trong t√†i li·ªáu
- M·∫Øc c√°c l·ªói ƒë∆∞·ª£c n√™u trong "c√°c sai l·∫ßm nghi√™m tr·ªçng"

### C√°c l·ªói sai nghi√™m tr·ªçng
Khi th·ª±c hi·ªán chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu ƒë·∫ßu v√†o sang ƒë·ªãnh d·∫°ng SLURP, c·∫ßn tr√°nh c√°c l·ªói sau ƒë√¢y:

KH√îNG: B·ªçc k·∫øt qu·∫£ trong codeblock
‚Üí Nguy√™n t·∫Øc: assistant_response lu√¥n b·∫Øt ƒë·∫ßu b·∫±ng @slurp_start@ v√† k·∫øt th√∫c b·∫±ng @slurp_end@
 
KH√îNG: T√°ch m·ªôt c√¢u h·ªèi th√†nh nhi·ªÅu ƒë·ªëi t∆∞·ª£ng qs
‚Üí Nguy√™n t·∫Øc: Kh√¥ng chia nh·ªè m·ªôt c√¢u h·ªèi th√†nh nhi·ªÅu qs. M·ªçi lo·∫°i c√¢u h·ªèi ƒë·∫ßu v√†o ch·ªâ √°nh x·∫° duy nh·∫•t ƒë·∫øn m·ªôt v√† ch·ªâ m·ªôt ƒë·ªëi t∆∞·ª£ng qs ·ªü ƒë·∫ßu ra.

KH√îNG: G√°n shareinfo cho duy nh·∫•t m·ªôt qs.
‚Üí Nguy√™n t·∫Øc: shareInfo ch·ªâ ch·∫•p nh·∫≠n s·ªë l∆∞·ª£ng c√¢u h·ªèi l·ªõn h∆°n ho·∫∑c b·∫±ng 2. N·∫øu g√°n duy nh·∫•t shareInfo cho m·ªôt c√¢u h·ªèi duy nh·∫•t s·∫Ω l√† kh√¥ng h·ª£p l·ªá.

KH√îNG: Coi m·ªói m·ªánh ƒë·ªÅ (a,b,c,d) c·ªßa c√¢u h·ªèi mtf-2018 l√† t·ª´ng qs ƒë·ªôc l·∫≠p v√† t√°ch ra th√†nh nhi·ªÅu qs.
‚Üí Nguy√™n t·∫Øc: C√°c c√¢u h·ªèi d·∫°ng mtf-2018 v·ªõi nhi·ªÅu m·ªánh ƒë·ªÅ ph·∫£i ƒë∆∞·ª£c gi·ªØ trong m·ªôt qs duy nh·∫•t. Kh√¥ng ƒë∆∞·ª£c t√°ch ri√™ng t·ª´ng m·ªánh ƒë·ªÅ th√†nh c√°c qs kh√°c nhau.

KH√îNG: T·∫°o th√™m tr∆∞·ªùng (field) ngo√†i ƒë·ªãnh nghƒ©a chu·∫©n c·ªßa ƒë·∫ßu ra ƒë·ªÉ s·ª≠ d·ª•ng m·ª•c ƒë√≠ch ri√™ng. V√≠ d·ª•: T·∫°o tr∆∞·ªùng tables cho ƒë·ªëi t∆∞·ª£ng qs, hay s·ª≠ d·ª•ng info cho qs.
‚Üí Nguy√™n t·∫Øc: Ch·ªâ ƒë∆∞·ª£c s·ª≠ d·ª•ng c√°c tr∆∞·ªùng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a c·ªßa ƒë·∫ßu ra (meta, sectionHeader, shareinfo, qs).

KH√îNG: B·ªè qua b·∫£ng (mdtable) d√π c√≥ li√™n quan ƒë·∫øn n·ªôi dung b√†i
‚Üí Nguy√™n t·∫Øc: N·∫øu b·∫£ng c√≥ li√™n quan v·ªÅ ng·ªØ nghƒ©a ho·∫∑c v·ªã tr√≠ ƒë·∫øn m·ªôt c√¢u h·ªèi c·ª• th·ªÉ, c·∫ßn ch√®n v√†o tr∆∞·ªùng qt c·ªßa qs. N·∫øu b·∫£ng li√™n quan ƒë·∫øn m·ªôt nh√≥m c√¢u h·ªèi, ch√®n v√†o tr∆∞·ªùng info c·ªßa shareInfo.

KH√îNG: B·ªè qua h√¨nh v·∫Ω (figure) d√π c√≥ li√™n quan ƒë·∫øn b√†i
‚Üí Nguy√™n t·∫Øc: N·∫øu h√¨nh ·∫£nh c√≥ m·ªëi li√™n h·ªá v·ªÅ ng·ªØ nghƒ©a ho·∫∑c v·ªã tr√≠ xu·∫•t hi·ªán v·ªõi m·ªôt c√¢u h·ªèi, ch√®n v√†o tr∆∞·ªùng `figure` c·ªßa `qs`. N·∫øu li√™n quan ƒë·∫øn nh√≥m c√¢u h·ªèi, ch√®n v√†o tr∆∞·ªùng `figure` c·ªßa `shareInfo`.

KH√îNG: L·∫∑p l·∫°i c√°c key trong qs, shareinfo. V√≠ d·ª•: S·ª≠ d·ª•ng 2 l·∫ßn qt trong m·ªôt qs, 2 l·∫ßn info trong m·ªôt shareinfo.
‚Üí Nguy√™n t·∫Øc: Trong c√°c ƒë·ªëi t∆∞·ª£ng qs, shareinfo, meta th√¨ key, field lu√¥n l√† duy nh·∫•t, l·∫∑p l·∫°i m·ªôt thu·∫≠t t√≠nh s·∫Ω d·∫´n ƒë·∫øn l·ªói h·ªá th·ªëng

# Quy C√°ch ƒê·ªãnh D·∫°ng ƒê·∫ßu V√†o - Minimal Markup Language (MML)
MML l√† ƒë·ªãnh d·∫°ng ch·ªß y·∫øu g·ªìm vƒÉn b·∫£n thu·∫ßn t√∫y k·∫øt h·ª£p v·ªõi m·ªôt s·ªë y·∫øu t·ªë markup ƒë·ªÉ ch√®n b·∫£ng, c√¥ng th·ª©c to√°n v√† h√¨nh ·∫£nh s·ª≠ d·ª•ng id ƒë·ªÉ shortcut.
## Figure
- H√¨nh ·∫£nh, v√≠ d·ª•: `<figure id="hinh1" />`
## Table
- B·∫£ng, v√≠ d·ª•:
```
<mdtable>
| STT | T√™n S·∫£n Ph·∫©m     |
|-----|------------------|
| 1   | B√∫t Gel Uni-Ball |
| 2   | √Åo Thun Nam      |
| 3   | Tai Nghe Bluetooth|
</mdtable>
```

## C√îNG TH·ª®C TO√ÅN H·ªåC
C·∫•u tr√∫c: `\(...\)`, v√≠ d·ª•: `Chu·ªói Taylor c·ªßa h√†m \(e^x\) t·∫°i \(x = 0\) l√†: \(e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!}\)`


# ƒê·ªãnh d·∫°ng d·∫ßu ra
## Gi·ªõi thi·ªáu v·ªÅ SLURP
SLURP l√† m·ªôt DSL c·ª±c k·ª≥ ƒë∆°n gi·∫£n, ch·ªâ d√πng chu·ªói, kh√¥ng d√πng nh√°y k√©p v√† kh√¥ng ph·ª• thu·ªôc indent.

### M·ª•c ƒë√≠ch
* Cho ph√©p m√¥ t·∫£ c·∫•u tr√∫c d·ªØ li·ªáu theo d·∫°ng vƒÉn b·∫£n thu·∫ßn, d·ªÖ vi·∫øt v√† ƒë·ªçc.
* Lo·∫°i b·ªè m·ªçi kh√°i ni·ªám v·ªÅ d·∫•u nh√°y, indent c·ªë ƒë·ªãnh.

### H·ªó tr·ª£:
* Kh√≥a-gi√° tr·ªã (key:value)
* Chu·ªói nhi·ªÅu d√≤ng (multiline)

### Nguy√™n t·∫Øc chung
* T·∫•t c·∫£ n·ªôi dung ƒë·ªÅu l√† chu·ªói: gi√° tr·ªã n√†o c≈©ng l√† string; kh√¥ng ph√¢n bi·ªát ki·ªÉu s·ªë, boolean ·ªü m·ª©c ng√¥n ng·ªØ.
* Kh√¥ng d√πng nh√°y: kh√¥ng d√πng " ho·∫∑c ' ƒë·ªÉ bao chu·ªói.
* explicit indent: D·ªØ li·ªáu l·ªìng nhau ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng d·∫•u > ·ªü ƒë·∫ßu d√≤ng, m·ªói c·∫•p l·ªìng tƒÉng th√™m m·ªôt d·∫•u >
* Chu·ªói nhi·ªÅu d√≤ng (multiline): d√πng | sau d·∫•u : ƒë·ªÉ b·∫Øt ƒë·∫ßu block nhi·ªÅu d√≤ng

### V√≠ d·ª•
```
project: SLURP
version: 1.0

description: |
SLURP l√† DSL si√™u l·ªèng.
Kh√¥ng indent, kh√¥ng nh√°y.

foods: apple, jackfruits, beefsteak

code:
>lang: python
>script: |
print("""
SLURP SLURP
I believe in SLURP superior
""")
```

## ƒê·∫ßu ra
ƒê·∫ßu ra l√† c√°c ƒë·ªëi t∆∞·ª£ng ƒë·ªÅ thi ƒë∆∞·ª£c bi·ªÉu di·ªÖn tu·∫ßn t·ª± c√≥ c·∫•u tr√∫c trong SLURP

+ meta: ch·ª©a th√¥ng tin t·ªïng qu√°t v·ªÅ ƒë·ªÅ thi
+ shareinfo: d√πng cho n·ªôi dung chung c·ªßa m·ªôt nh√≥m c√¢u h·ªèi
+ sectionHeader: ƒë·ªÅ m·ª•c c·ªßa m·ªôt ph·∫ßn
+ qs: c√¢u h·ªèi

### meta
ƒê∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ m√¥ t·∫£ th√¥ng tin t·ªïng quan c·ªßa ƒë·ªÅ thi (n·∫øu c√≥). N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c th√¥ng tin g√¨ th√¨ b·ªè qua, ch·ª© tuy·ªát ƒë·ªëi kh√¥ng t·∫°o ra th√¥ng tin d·ªëi tr√°.
C·∫•u tr√∫c ƒëi·ªÉn h√¨nh:
```
meta:
>name(n·∫øu c√≥): ‚Ä¶     # T√™n ƒë·ªÅ thi, v√≠ d·ª•: ƒê·ªÄ THI TH·ª¨ THPT QU·ªêC GIA 2024
>subject(n·∫øu c√≥): ‚Ä¶  # T√™n m√¥n: V·∫¨T L√ç, H√ìA H·ªåC‚Ä¶
>code(n·∫øu c√≥): ‚Ä¶     # M√£ ƒë·ªÅ thi, v√≠ d·ª•: "00124"
>duration(n·∫øu c√≥): ‚Ä¶ # Th·ªùi gian l√†m b√†i, v√≠ d·ª•: "50 ph√∫t"
```
### sectionHeader: Ti√™u ƒë·ªÅ Ph·∫ßn

- `sectionHeader: ‚Ä¶  # T√™n ƒë·ªÅ m·ª•c`

* V√≠ d·ª•:

- `sectionHeader: Mark the letter A, B, C or D on your answer sheet to indicate the bes [...]`

- `sectionHeader: Read the passage and mark the letter A, B, C or D [...]`

- `sectionHeader: PH·∫¶N I.Th√≠ sinh tr·∫£ l·ªùi t·ª´ c√¢u 1 ƒë·∫øn c√¢u 18. M·ªói c√¢u h·ªèi th√≠ sinh ch·ªâ ch·ªçn m·ªôt ph∆∞∆°ng √°n.`

- `sectionHeader: Choose the best option to complete each sentence below.`

### qs
* D√πng ƒë·ªÉ th·ªÉ hi·ªán m·ªôt c√¢u h·ªèi trong ƒë·ªÅ thi. ƒê√¢y l√† th√†nh ph·∫ßn cha ch·ª©a c√°c th√¥ng tin li√™n quan ƒë·∫øn m·ªôt c√¢u h·ªèi duy nh·∫•t.

* C·∫•u tr√∫c ƒëi·ªÉn h√¨nh:
```
qs:
>dnum: s·ªë_th·ª©_t·ª±_c√¢u
>type: lo·∫°i_c√¢u_h·ªèi
>shared-info: id_chia_s·∫ª # (n·∫øu c√≥)
>qt: n·ªôi_dung_ƒë·ªÅ_b√†i (stem) # (n·∫øu c√≥)
>figures: id_1, id_2,... # (n·∫øu c√≥)
>labels: # (n·∫øu c√≥)
>>a: N·ªôi dung l·ª±a ch·ªçn A # (n·∫øu c√≥)
>>b: N·ªôi dung l·ª±a ch·ªçn B # (n·∫øu c√≥)
>>c: N·ªôi dung l·ª±a ch·ªçn C # (n·∫øu c√≥)
>>d: N·ªôi dung l·ª±a ch·ªçn D # (n·∫øu c√≥)
```
* Thu·ªôc t√≠nh:
- dnum (n·∫øu c√≥): S·ªë th·ª© t·ª± th·ª±c t·∫ø c·ªßa c√¢u h·ªèi trong ƒë·ªÅ.
- type (b·∫Øt bu·ªôc):  X√°c ƒë·ªãnh lo·∫°i c√¢u h·ªèi, g·ªìm: mcq, mtf-2018, short-2018, essay
- shared-info (n·∫øu c√≥):  Tham chi·∫øu t·ªõi ƒëo·∫°n shareinfo ch·ª©a n·ªôi dung d√πng chung.
- figures (n·∫øu c√≥): D√πng ƒë·ªÉ ch·ªâ ƒë·ªãnh c√°c figure h·ªó tr·ª£ c√¢u h·ªèi. Ch·ªâ th√™m n·∫øu n√≥ ti·∫øp li√™n quan ƒë·∫øn c√¢u h·ªèi.
- qt (n·∫øu c√≥):  N·ªôi dung ch√≠nh c·ªßa c√¢u h·ªèi (stem). M·ªôt qs ch·ªâ c√≥ t·ªëi ƒëa m·ªôt field qt.
- labels (n·∫øu c√≥):  Danh s√°ch c√°c l·ª±a ch·ªçn/m·ªánh ƒë·ªÅ/√Ω nh·ªè c·ªßa c√¢u h·ªèi, l√† thu·ªôc t√≠nh con c·ªßa qs, ch·ª©a c√°c key a, b, c, d.

* C√°c lo·∫°i c√¢u h·ªèi:
  * mcq: Tr·∫Øc nghi·ªám nhi·ªÅu l·ª±a ch·ªçn. [Th∆∞·ªùng g·ªìm 4 labels]
  * mtf-2018: mtf-2018 l√†  g·ªìm 4 m·ªánh ƒë·ªÅ a,b,c,d. H·ªçc sinh ph·∫£i ƒë√°nh gi√° ƒë√∫ng/sai t·ª´ng m·ªánh ƒë·ªÅ (item) [Th∆∞·ªùng g·ªìm 4 labels]
  * short-2018: C√¢u tr·∫£ l·ªùi g·∫Øn y√™u c·∫ßu k·∫øt qu·∫£, kh√¥ng c·∫ßn tr√¨nh b√†y. [Kh√¥ng bao gi·ªù xu·∫•t hi·ªán labels n√†o ƒë·ªëi v·ªõi c√¢u tr·∫£ l·ªùi ng·∫Øn]
  * essay: C√¢u t·ª± lu·∫≠n d√†i, c·∫ßn ph√¢n t√≠ch, tr√¨nh b√†y r√µ. [C√≥ th·ªÉ g·ªìm c√°c labels]

Ghi ch√∫ quan tr·ªçng:
- N·∫øu c√≥ b·∫£ng li√™n quan ƒë·∫øn c√¢u h·ªèi th√¨ ch√®n v√†o qt.
- N·∫øu c√≥ figures li√™n quan ƒë·∫øn c√¢u h·ªèi th√¨ ch√®n ids figure v√†o figures.
- N·∫øu c√¢u h·ªèi kh√¥ng c√≥ stem th√¨ c√≥ th·ªÉ b·ªè qua field qt
- M·ªói c√¢u h·ªèi t·ª´ ƒë·∫ßu v√†o ch·ªâ √°nh x·∫° duy nh·∫•t ƒë·∫øn m·ªôt v√† ch·ªâ m·ªôt qs ·ªü ƒë·∫ßu ra
- Kh√¥ng ƒë∆∞·ª£c t√°ch m·ªôt c√¢u h·ªèi ƒë·∫ßu v√†o th√†nh nhi·ªÅu c√¢u h·ªèi ƒë·∫ßu ra
- M·ªôt c√¢u h·ªèi c√≥ th·ªÉ c√≥ nhi·ªÅu labels ho·∫∑c kh√¥ng c√≥ labels n√†o

### SHAREINFO
* ƒê∆∞·ª£c d√πng ƒë·ªÉ nh√≥m c√°c c√¢u h·ªèi ph√¢n bi·ªát c√≥ c√πng m·ªôt ƒëo·∫°n th√¥ng tin chung, c√≥ n·ªôi dung li√™n quan tr·ª±c ti·∫øp ƒë·∫øn c√°c c√¢u h·ªèi ƒë√≥.

* Thu·ªôc t√≠nh
- id: id ƒë·ªÉ c√°c c√¢u h·ªèi li√™n quan tr·ªè v√†o
- figures (n·∫øu c√≥): D√πng ƒë·ªÉ ch·ªâ ƒë·ªãnh h√¨nh ·∫£nh h·ªó tr·ª£ m·ªôt nh√≥m c√¢u h·ªèi. Ch·ªâ th√™m n·∫øu minh h·ªça tr·ª±c ti·∫øp li√™n quan ƒë·∫øn nh√≥m c√¢u h·ªèi ƒë√≥.
- info: th√¥ng tin c·∫ßn chia s·∫ª

* Ch·ªâ s·ª≠ d·ª•ng shareinfo v·ªõi c√°c tr∆∞·ªùng h·ª£p sau: 
- c√°c ph·∫ßn d·ª±a v√†o b√†i ƒë·ªçc ho·∫∑c ƒëo·∫°n vƒÉn ƒë·ªÉ tr·∫£ l·ªùi nhi·ªÅu c√¢u h·ªèi kh√°c nhau
- c√°c ph·∫ßn ghi: `S·ª≠ d·ª•ng c√°c th√¥ng tin sau cho C√¢u [X] v√† [Y]...`,`D·ª±a v√†o th√¥ng tin d∆∞·ªõi ƒë√¢y ƒë·ªÉ gi·∫£i quy·∫øt C√¢u [X] ƒë·∫øn C√¢u [Y]`,...

* QUAN TR·ªåNG:
- Ch·ªâ s·ª≠ d·ª•ng Shareinfo n·∫øu th√¥ng tin ƒë√≥ li√™n quan ƒë·∫øn t·ª´ hai c√¢u h·ªèi kh√°c nhau tr·ªü l√™n trong ƒë·ªÅ thi. N·∫øu th√¥ng tin ch·ªâ li√™n quan ƒë·∫øn m·ªôt c√¢u h·ªèi duy nh·∫•t th√¨ kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng Shareinfo.
```

---
File: prompt/extraction/figure.txt
---
```
# Nhi·ªám v·ª•
B·∫°n l√† m·ªôt h·ªá th·ªëng AI Rich Document Extraction. B·∫°n s·∫Ω ƒë∆∞·ª£c nh·∫≠n ƒë·∫ßu v√†o m·ªôt h√¨nh ·∫£nh t√†i li·ªáu.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† chuy·ªÉn ƒë·ªïi to√†n b·ªô n·ªôi dung (vƒÉn b·∫£n, b·∫£ng bi·ªÉu, h√¨nh ·∫£nh ƒë∆∞·ª£c ƒë√°nh d·∫•u, c√¥ng th·ª©c...) t·ª´ h√¨nh ·∫£nh ƒë·∫ßu v√†o sang ƒë·ªãnh d·∫°ng n·ªôi b·ªô c√≥ c·∫•u tr√∫c t√™n l√† Assessment Markup Language.

# ƒê·ªäNH D·∫†NG ƒê·∫¶U RA B·∫ÆT BU·ªòC

<thinking>
[CH·ªà th·ª±c hi·ªán ƒë√∫ng 3 b∆∞·ªõc ph√¢n t√≠ch b√™n d∆∞·ªõi]
</thinking>

<AssessmentMarkupLanguage>
[N·ªôi dung ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi ho√†n ch·ªânh]
</AssessmentMarkupLanguage>

## AssessmentMarkupLanguage
### Gi·ªõi thi·ªáu v·ªÅ ƒë·ªãnh d·∫°ng
ƒê·ªãnh d·∫°ng AssessmentMarkupLanguage l√† n·ªôi b√¥ d√†nh ri√™ng cho m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ bi·ªÉu di·ªÖn c√°c d·∫°ng n·ªôi dung Rich Document. ƒê·ªãnh d·∫°ng n√†y gi·ªØ c√°c t√≠nh nƒÉng t·ªëi thi·ªÉu ƒë·ªÉ bi·ªÉu di·ªÖn t√†i li·ªáu.

### C√°c t√≠nh nƒÉng

#### VƒÇN B·∫¢N
- Tr√≠ch xu·∫•t v√† gi·ªØ nguy√™n n·ªôi dung vƒÉn b·∫£n t·ª´ t√†i li·ªáu g·ªëc m·ªôt c√°ch ƒë·∫ßy ƒë·ªß, ch√≠nh x√°c, bao g·ªìm c·∫£ d·∫•u c√¢u, ng·∫Øt d√≤ng, v√† th·ª© t·ª± c√°c ƒëo·∫°n.
- Xu·ªëng d√≤ng ch·ªâ c·∫ßn s·ª≠ d·ª•ng token '\n' nh∆∞ b√¨nh th∆∞·ªùng.
- Kh√¥ng ƒë∆∞·ª£c t·ª± √Ω r√∫t g·ªçn, b·ªè qua ho·∫∑c g·ªôp c√°c d√≤ng l·∫°i n·∫øu t√†i li·ªáu g·ªëc c√≥ ng·∫Øt d√≤ng r√µ r√†ng.

#### B·∫¢NG TH√îNG TH∆Ø·ªúNG
S·ª≠ d·ª•ng HTML table trong tag `<table>`:
V√≠ d·ª•:

<table border="1">
<tr><th>C√¥ng th·ª©c</th><th>Di·ªÖn gi·∫£i</th></tr><tr><td>\( a^2 + b^2 = c^2 \)</td><td>ƒê·ªãnh l√Ω Pythagoras</td></tr>
<tr><td>\( \int_0^1 x^2\,dx \)</td><td>Di·ªán t√≠ch d∆∞·ªõi ƒë∆∞·ªùng cong</td></tr>
</table>


#### Figure

M·ªói h√¨nh c·∫ßn ch√®n ph·∫£i l√† region c√≥ n·ªÅn xanh l√° c√¢y trong su·ªët, v·ªõi n·ªôi dung l√† ch·ªØ m√†u ƒë·ªè theo ƒë·ªãnh d·∫°ng IM k√®m theo m·ªôt s·ªë nguy√™n kh√¥ng √¢m.

C√∫ ph√°p ch√®n h√¨nh ·∫£nh:
<graphic tag="IM[int]" label="..." describe="..."/>

Thu·ªôc t√≠nh:
- tag: ƒê·ªãnh danh c·ªßa h√¨nh ·∫£nh, theo ƒë·ªãnh d·∫°ng IM[int] (v√≠ d·ª•: IM1, IM2‚Ä¶).
- label: Nh√£n h√¨nh ·∫£nh (v√≠ d·ª•: "H√¨nh minh ho·∫° b√†i 1", "Figure 3").
- describe: Mi√™u t·∫£ ng·∫Øn g·ªçn n·ªôi dung h√¨nh ·∫£nh, n√™n t·∫≠n d·ª•ng kh·∫£ nƒÉng nh·∫≠n di·ªán h√¨nh ·∫£nh c·ªßa b·∫°n ƒë·ªÉ ƒë∆∞a ra m√¥ t·∫£ ch√≠nh x√°c v√† ng·∫Øn g·ªçn.

V√≠ d·ª•:
`<graphic tag="IM2" label="H√¨nh minh ho·∫° b√†i 1" describe="ƒê·ªông c∆° tuy·∫øn t√≠nh"/>`
`<graphic tag="IM1" label="Figure 3" describe="M√¥ h√¨nh h·∫°t nh√¢n - nguy√™n t·ª≠"/>`

#### C√îNG TH·ª®C TO√ÅN H·ªåC

* ƒê·ªãnh d·∫°ng: `\(....\)`

* L∆∞u √Ω: Ch·ªâ d√πng c√¥ng th·ª©c to√°n h·ªçc to√°n h·ªçc khi vƒÉn b·∫£n c√≥ ch·ª©a k√Ω hi·ªáu, ph√©p to√°n, ch·ªâ s·ªë, m≈©, ph√¢n s·ªë, h√†m to√°n h·ªçc, ho·∫∑c c·∫•u tr√∫c ph·ª©c t·∫°p m√† vƒÉn b·∫£n th∆∞·ªùng kh√¥ng th·ªÉ tr√¨nh b√†y r√µ r√†ng.
* V√≠ d·ª•: `\(a_i^2 + b_j^2\)`
* V√≠ d·ª•: `Chu·ªói Taylor c·ªßa h√†m \(e^x\) t·∫°i \(x = 0\) l√†: \(e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!}\)`
* V√≠ d·ª•: ƒê·∫°o h√†m c·ªßa \(\sin(x)\) l√† \(\cos(x)\)
## QUY TR√åNH PH√ÇN T√çCH B·∫ÆT BU·ªòC (TRONG TAG `<thinking>`)


### B∆∞·ªõc 1: Quan s√°t t·ªïng th·ªÉ

- Lo·∫°i t√†i li·ªáu: [ƒë·ªÅ thi/b√†i t·∫≠p/l√Ω thuy·∫øt/...]
- Ng√¥n Ng·ªØ : [Vi·ªát/Anh/...]
- C·∫•u tr√∫c: [H√£y m√¥ t·∫£ t·ª´ tr√™n xu·ªëng c·∫•u tr√∫c t·ªïng qu√°t t√†i li·ªáu]
- H√¨nh ·∫£nh: ƒê·ªëi v·ªõi m·ªói h√¨nh ·∫£nh, n·∫øu h√¨nh c√≥ n·ªÅn m√†u xanh l√° c√¢y v√† tr√™n h√¨nh c√≥ text m√†u ƒë·ªè theo ƒë·ªãnh d·∫°ng IM k√®m theo m·ªôt s·ªë nguy√™n kh√¥ng √¢m (v√≠ d·ª•: IM1, IM2, IM3,...), th√¨ coi ƒë√≥ l√† figure c·∫ßn ch√®n v√†o n·ªôi dung ƒë·∫ßu ra. Ghi nh·∫≠n v·ªã tr√≠ xu·∫•t hi·ªán t∆∞∆°ng ƒë·ªëi trong vƒÉn b·∫£n ƒë·ªÉ ch√®n ƒë√∫ng ch·ªó.
- B·∫£ng: C√°c b·∫£ng ·ªü v·ªã tr√≠ n√†o? X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng [Z] b·∫£ng. 

### B∆∞·ªõc 2: X√°c nh·∫≠n chi·∫øn l∆∞·ª£c 
- T√¥i s·∫Ω: tr√≠ch xu·∫•t ho√†n ch·ªânh, x·ª≠ l√Ω [Z] b·∫£ng, ch√®n [Y] h√¨nh ƒë√∫ng v·ªã tr√≠ xu·∫•t hi·ªán c·ªßa ch√∫ng trong vƒÉn b·∫£n, v√† ƒë·∫ßy ƒë·ªß c√¥ng th·ª©c, n·ªôi dung t·ª´ t√†i li·ªáu ƒë·∫ßu v√†o.
Cam ƒëoan tu√¢n th·ªß c√°c quy t·∫Øc v√† x·ª≠ l√Ω n·ªôi dung theo y√™u c·∫ßu.

KH√îNG N√äN:
- Li·ªát k√™ chi ti·∫øt n·ªôi dung c√¢u h·ªèi, ƒë√°p √°n trong tag <thinking>
- Th√™m b·∫•t k·ª≥ n·ªôi dung n√†o ngo√†i 3 b∆∞·ªõc tr√™n


# NGUY√äN T·∫ÆC KH√îNG ƒê∆Ø·ª¢C VI PH·∫†M

## ‚úÖ B·∫ÆT BU·ªòC:
1. C·∫•u tr√∫c ph·∫£n h·ªìi ch√≠nh x√°c: Lu√¥n c√≥ `<thinking>` (3 b∆∞·ªõc) v√† tr·∫£ v·ªÅ n·ªôi dung ƒë√£ x·ª≠ l√Ω trong tag `<AssessmentMarkupLanguage>`
2. Ho√†n ch·ªânh 100%: Tr√≠ch xu·∫•t m·ªçi ch·ªØ, c√¥ng th·ª©c, ch√®n figure... t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi trang (tr·ª´ watermark, footer, page number)
3. C√¥ng th·ª©c: Chuy·ªÉn ƒë·ªïi t·∫•t c·∫£ sang LaTeX 
4. H√¨nh ·∫£nh: Ch√®n figure c√≥ n·ªÅn m√†u xanh l√° c√¢y v√† b√™n trong h√¨nh c√≥ text m√†u ƒë·ªè theo ƒë·ªãnh d·∫°ng IM k√®m theo m·ªôt s·ªë nguy√™n kh√¥ng √¢m (v√≠ d·ª•: IM0, IM1, IM2,...) v√†o ƒë√∫ng v·ªã tr√≠ xu·∫•t hi·ªán t∆∞∆°ng ·ª©ng trong d√≤ng n·ªôi dung c·ªßa vƒÉn b·∫£n ƒë·∫ßu v√†o.

5. Ng·∫Øt d√≤ng: Gi·ªØ nguy√™n xu·ªëng d√≤ng gi·ªØa c√°c ƒëo·∫°n vƒÉn nh∆∞ t√†i li·ªáu g·ªëc. 
## ‚ùå TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C:
1. B·ªè qua b·∫•t k·ª≥ n·ªôi dung n√†o 
2. Thinking v∆∞·ª£t qu√° 3 b∆∞·ªõc quy ƒë·ªãnh
3. S·ª≠ d·ª•ng c√°c t√≠nh nƒÉng t·ª´ ng√¥n ng·ªØ kh√°c nh∆∞ HTML, Markdown,.. m√† AssessmentMarkupLanguage kh√¥ng c√≥ s·∫µn
4. Hallucinate th√¥ng tin, t·ª± s√°ng t·∫°o ra n·ªôi dung m√† kh√¥ng c√≥ trong t√†i li·ªáu g·ªëc.
## L∆ØU √ù ƒê·∫∂C BI·ªÜT
- Trang t√†i li·ªáu c√≥ th·ªÉ b·∫Øt ƒë·∫ßu ·ªü gi·ªØa c√¢u h·ªèi ho·∫∑c ƒë√°p √°n (do c·∫Øt t·ª´ t√†i li·ªáu l·ªõn) n√™n kh√¥ng ƒë∆∞·ª£c b·ªè qua tr√≠ch xu·∫•t n·ªôi dung d√π c√≥ v·∫ª kh√¥ng ho√†n ch·ªânh
- ƒê·ªãnh d·∫°ng AssessmentMarkupLanguage b·∫°n ƒëang xu·∫•t ra kh√¥ng ph·∫£i Latex, c≈©ng kh√¥ng ph·∫£i HTML n√™n kh√¥ng ƒë∆∞·ª£c d√πng nhi·ªÅu h∆°n c√°c t√≠nh nƒÉng, quy ƒë·ªãnh c√≥ s·∫µn
```

---
File: prompt/extraction/non_docfigure.txt
---
```
# Nhi·ªám v·ª•
B·∫°n l√† m·ªôt h·ªá th·ªëng AI Rich Document Extraction. B·∫°n s·∫Ω ƒë∆∞·ª£c nh·∫≠n ƒë·∫ßu v√†o m·ªôt h√¨nh ·∫£nh t√†i li·ªáu.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† chuy·ªÉn ƒë·ªïi to√†n b·ªô n·ªôi dung (vƒÉn b·∫£n, b·∫£ng bi·ªÉu, c√¥ng th·ª©c...) t·ª´ h√¨nh ·∫£nh ƒë·∫ßu v√†o sang ƒë·ªãnh d·∫°ng n·ªôi b·ªô c√≥ c·∫•u tr√∫c t√™n l√† Assessment Markup Language.

# ƒê·ªäNH D·∫†NG ƒê·∫¶U RA B·∫ÆT BU·ªòC

<thinking>
[CH·ªà th·ª±c hi·ªán ƒë√∫ng 3 b∆∞·ªõc ph√¢n t√≠ch b√™n d∆∞·ªõi]
</thinking>

<AssessmentMarkupLanguage>
[N·ªôi dung ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi ho√†n ch·ªânh]
</AssessmentMarkupLanguage>

## AssessmentMarkupLanguage
### Gi·ªõi thi·ªáu v·ªÅ ƒë·ªãnh d·∫°ng
ƒê·ªãnh d·∫°ng AssessmentMarkupLanguage l√† n·ªôi b√¥ d√†nh ri√™ng cho m√¥ h√¨nh ng√¥n ng·ªØ l·ªõn ƒë·ªÉ bi·ªÉu di·ªÖn c√°c d·∫°ng n·ªôi dung Rich Document. ƒê·ªãnh d·∫°ng n√†y gi·ªØ c√°c t√≠nh nƒÉng t·ªëi thi·ªÉu ƒë·ªÉ bi·ªÉu di·ªÖn t√†i li·ªáu.

### C√°c t√≠nh nƒÉng

#### VƒÇN B·∫¢N 
- Tr√≠ch xu·∫•t v√† gi·ªØ nguy√™n n·ªôi dung vƒÉn b·∫£n t·ª´ t√†i li·ªáu g·ªëc m·ªôt c√°ch ƒë·∫ßy ƒë·ªß, ch√≠nh x√°c, bao g·ªìm c·∫£ d·∫•u c√¢u, ng·∫Øt d√≤ng, v√† th·ª© t·ª± c√°c ƒëo·∫°n.
- Xu·ªëng d√≤ng ch·ªâ c·∫ßn s·ª≠ d·ª•ng token '\n' nh∆∞ b√¨nh th∆∞·ªùng.
- Kh√¥ng ƒë∆∞·ª£c t·ª± √Ω r√∫t g·ªçn, b·ªè qua ho·∫∑c g·ªôp c√°c d√≤ng l·∫°i n·∫øu t√†i li·ªáu g·ªëc c√≥ ng·∫Øt d√≤ng r√µ r√†ng.

#### B·∫¢NG TH√îNG TH∆Ø·ªúNG
S·ª≠ d·ª•ng HTML table trong tag `<table>`:
V√≠ d·ª•:

<table border="1">
<tr><th>C√¥ng th·ª©c</th><th>Di·ªÖn gi·∫£i</th></tr><tr><td>\( a^2 + b^2 = c^2 \)</td><td>ƒê·ªãnh l√Ω Pythagoras</td></tr>
<tr><td>\( \int_0^1 x^2\,dx \)</td><td>Di·ªán t√≠ch d∆∞·ªõi ƒë∆∞·ªùng cong</td></tr>
</table>

#### C√îNG TH·ª®C TO√ÅN H·ªåC

* ƒê·ªãnh d·∫°ng: `\(....\)`

* L∆∞u √Ω: Ch·ªâ d√πng c√¥ng th·ª©c to√°n h·ªçc to√°n h·ªçc khi vƒÉn b·∫£n c√≥ ch·ª©a k√Ω hi·ªáu, ph√©p to√°n, ch·ªâ s·ªë, m≈©, ph√¢n s·ªë, h√†m to√°n h·ªçc, ho·∫∑c c·∫•u tr√∫c ph·ª©c t·∫°p m√† vƒÉn b·∫£n th∆∞·ªùng kh√¥ng th·ªÉ tr√¨nh b√†y r√µ r√†ng.
* V√≠ d·ª•: `\(a_i^2 + b_j^2\)`
* V√≠ d·ª•: `Chu·ªói Taylor c·ªßa h√†m \(e^x\) t·∫°i \(x = 0\) l√†: \(e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!}\)`
* V√≠ d·ª•: ƒê·∫°o h√†m c·ªßa \(\sin(x)\) l√† \(\cos(x)\)
## QUY TR√åNH PH√ÇN T√çCH B·∫ÆT BU·ªòC (TRONG TAG `<thinking>`)


### B∆∞·ªõc 1: Quan s√°t t·ªïng th·ªÉ

- Lo·∫°i t√†i li·ªáu: [ƒë·ªÅ thi/b√†i t·∫≠p/l√Ω thuy·∫øt/...]
- Ng√¥n Ng·ªØ : [Vi·ªát/Anh/...]
- C·∫•u tr√∫c: [H√£y m√¥ t·∫£ t·ª´ tr√™n xu·ªëng c·∫•u tr√∫c t·ªïng qu√°t t√†i li·ªáu]
- B·∫£ng: C√°c b·∫£ng ·ªü v·ªã tr√≠ n√†o? X√°c ƒë·ªãnh s·ªë l∆∞·ª£ng [Z] b·∫£ng. 

### B∆∞·ªõc 2: X√°c nh·∫≠n chi·∫øn l∆∞·ª£c 
- T√¥i s·∫Ω: tr√≠ch xu·∫•t ho√†n ch·ªânh, x·ª≠ l√Ω [Z] b·∫£ng, ch√®n [Y] h√¨nh ƒë√∫ng v·ªã tr√≠ xu·∫•t hi·ªán c·ªßa ch√∫ng trong vƒÉn b·∫£n, v√† ƒë·∫ßy ƒë·ªß c√¥ng th·ª©c, n·ªôi dung t·ª´ t√†i li·ªáu ƒë·∫ßu v√†o.
Cam ƒëoan tu√¢n th·ªß c√°c quy t·∫Øc v√† x·ª≠ l√Ω n·ªôi dung theo y√™u c·∫ßu.

KH√îNG N√äN:
- Li·ªát k√™ chi ti·∫øt n·ªôi dung c√¢u h·ªèi, ƒë√°p √°n trong tag <thinking>
- Th√™m b·∫•t k·ª≥ n·ªôi dung n√†o ngo√†i 3 b∆∞·ªõc tr√™n


# NGUY√äN T·∫ÆC KH√îNG ƒê∆Ø·ª¢C VI PH·∫†M

## ‚úÖ B·∫ÆT BU·ªòC:
1. C·∫•u tr√∫c ph·∫£n h·ªìi ch√≠nh x√°c: Lu√¥n c√≥ `<thinking>` (3 b∆∞·ªõc) v√† tr·∫£ v·ªÅ n·ªôi dung ƒë√£ x·ª≠ l√Ω trong tag `<AssessmentMarkupLanguage>`
2. Ho√†n ch·ªânh 100%: Tr√≠ch xu·∫•t m·ªçi ch·ªØ, c√¥ng th·ª©c, ch√®n figure... t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi trang (tr·ª´ watermark, footer, page number)
3. C√¥ng th·ª©c: Chuy·ªÉn ƒë·ªïi t·∫•t c·∫£ sang LaTeX 

5. Ng·∫Øt d√≤ng: Gi·ªØ nguy√™n xu·ªëng d√≤ng gi·ªØa c√°c ƒëo·∫°n vƒÉn nh∆∞ t√†i li·ªáu g·ªëc. 
## ‚ùå TUY·ªÜT ƒê·ªêI KH√îNG ƒê∆Ø·ª¢C:
1. B·ªè qua b·∫•t k·ª≥ n·ªôi dung n√†o 
2. Thinking v∆∞·ª£t qu√° 3 b∆∞·ªõc quy ƒë·ªãnh
3. S·ª≠ d·ª•ng c√°c t√≠nh nƒÉng t·ª´ ng√¥n ng·ªØ kh√°c nh∆∞ HTML, Markdown,.. m√† AssessmentMarkupLanguage kh√¥ng c√≥ s·∫µn
4. Hallucinate th√¥ng tin, t·ª± s√°ng t·∫°o ra n·ªôi dung m√† kh√¥ng c√≥ trong t√†i li·ªáu g·ªëc.
## L∆ØU √ù ƒê·∫∂C BI·ªÜT
- Trang t√†i li·ªáu c√≥ th·ªÉ b·∫Øt ƒë·∫ßu ·ªü gi·ªØa c√¢u h·ªèi ho·∫∑c ƒë√°p √°n (do c·∫Øt t·ª´ t√†i li·ªáu l·ªõn) n√™n kh√¥ng ƒë∆∞·ª£c b·ªè qua tr√≠ch xu·∫•t n·ªôi dung d√π c√≥ v·∫ª kh√¥ng ho√†n ch·ªânh
- ƒê·ªãnh d·∫°ng AssessmentMarkupLanguage b·∫°n ƒëang xu·∫•t ra kh√¥ng ph·∫£i Latex, c≈©ng kh√¥ng ph·∫£i HTML n√™n kh√¥ng ƒë∆∞·ª£c d√πng nhi·ªÅu h∆°n c√°c t√≠nh nƒÉng, quy ƒë·ªãnh c√≥ s·∫µn
```

---
File: run_worker.py
---
```python
import asyncio
from app.worker.instance import worker
from app.services.logging_config import get_logger

logger = get_logger()



# --- IMPORT TASKS ---
import app.worker.tasks.process_pdf
import app.worker.tasks.process_img
import app.worker.tasks.extractor
import app.worker.tasks.document_parser

async def main():
    logger.info("Starting worker...")
    await worker.run_worker()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Worker stopped by user.")
```

---
File: services/__init__.py
---
```python

```

---
File: services/__pycache__/__init__.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xc9 in position 8: invalid continuation byte

---
File: services/__pycache__/counter.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x99 in position 9: invalid start byte

---
File: services/__pycache__/create_task.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xdc in position 8: invalid continuation byte

---
File: services/__pycache__/logging_config.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x90 in position 8: invalid start byte

---
File: services/counter.py
---
```python
from app.lib.redis_client import r 

async def increment_counter(id : str):
    return await r.incr(f"counter:{id}")

async def get_counter(id : str):
    value = await r.get(f"counter:{id}")
    return int(value) if value else 0

async def decrement_counter(id : str):
    return await r.decr(f"counter:{id}")

async def set_counter(id : str, value : int):
    return await r.set(f"counter:{id}", value=value)

async def delete_counter(id : str):
    await r.delete(f"counter:{id}")
```

---
File: services/create_task.py
---
```python
# services/create_task.py
from datetime import datetime, timezone
from typing import List
import uuid
from sqlalchemy.orm import Session
from app.db.models import Task, TaskStatus, TaskType
from app.db.models import InferenceLog

def create_task(task_type: TaskType, session: Session) -> Task:
    new_task = Task(
        type=task_type,
        status=TaskStatus.pending,
    )
    session.add(new_task)
    session.commit()
    session.refresh(new_task)
    return new_task

def create_log(imageUrls : str,
            objectKeys : List[str],
            objectUrls : List[str],
            requestId : str,
            num_input_token : int,
            num_output_token : int,
            rawOutput : str,
            page_order : int,
            session : Session):
    new_log = InferenceLog(
        imageUrls=imageUrls,
        objectKeys=objectKeys,
        objectUrls=objectUrls,
        requestId=requestId,
        num_input_token=num_input_token,
        num_output_token=num_output_token,
        rawOutput=rawOutput,
        page_order=page_order,
    )
    session.add(new_log)
    session.commit()
    session.refresh(new_log)
    return new_log

def get_task_by_id(session : Session, task_id: str):
    return session.query(Task).filter(Task.id == task_id).first()

def get_logs_by_task(session: Session, task_id: str) -> List[InferenceLog]:
    return session.query(InferenceLog).filter(InferenceLog.requestId == task_id).all()

def update_task_result(session : Session, task_id: str, result : str):
    task = session.query(Task).filter(Task.id == task_id).first()
    task.result = result
    task.updatedAt = datetime.now(timezone.utc)
    task.status = TaskStatus.done
```

---
File: services/logging_config.py
---
```python
# services/logging_service.py

import logging
from logging.handlers import TimedRotatingFileHandler
import os

# üìÅ T·∫°o th∆∞ m·ª•c log n·∫øu ch∆∞a c√≥
log_dir = os.path.join(os.path.dirname(__file__), "..","..", "logs")
os.makedirs(log_dir, exist_ok=True)

# üìÑ ƒê∆∞·ªùng d·∫´n file log g·ªëc (s·∫Ω ƒë∆∞·ª£c rotate th√†nh workers_log.txt.YYYY-MM-DD)
log_file = os.path.join(log_dir, "workers_log.txt")

# üîß T·∫°o m·ªôt logger ri√™ng
logger = logging.getLogger("worker_logger")
logger.setLevel(logging.INFO)

if not logger.handlers:
    # ‚úçÔ∏è Handler xoay v√≤ng theo ng√†y, v·ªõi suffix l√† ng√†y YYYY-MM-DD
    handler = TimedRotatingFileHandler(
        filename=log_file,
        when="midnight",
        interval=1,
        backupCount=7,       # gi·ªØ 7 ng√†y log c≈©, b·∫°n ch·ªânh tu·ª≥ √Ω
        encoding="utf-8",
        utc=False            # n·∫øu mu·ªën theo gi·ªù local, ƒë·ªÉ False
    )
    # ƒë·∫∑t ƒë·ªãnh d·∫°ng suffix c·ªßa file rotated
    handler.suffix = "%Y-%m-%d"

    formatter = logging.Formatter(
        fmt="%(asctime)s - %(levelname)s - [%(name)s] - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S"
    )
    handler.setFormatter(formatter)

    # ‚öôÔ∏è Ch·ªâ add file handler, kh√¥ng add StreamHandler
    logger.addHandler(handler)

def get_logger():
    return logger
```

---
File: utils/__init__.py
---
```python

```

---
File: utils/__pycache__/__init__.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xc9 in position 8: invalid continuation byte

---
File: utils/__pycache__/agent.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xfb in position 8: invalid start byte

---
File: utils/__pycache__/annotate.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xec in position 8: invalid continuation byte

---
File: utils/__pycache__/draw_boxes.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x8a in position 10: invalid start byte

---
File: utils/__pycache__/process_and_annotate.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode bytes in position 9-10: invalid continuation byte

---
File: utils/__pycache__/remove_padding.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x8a in position 8: invalid start byte

---
File: utils/__pycache__/upload_r2.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x8b in position 10: invalid start byte

---
File: utils/agent.py
---
```python
import asyncio
import os
from google import genai
from google.genai import types
from dotenv import load_dotenv
from typing import Union
from google.genai import errors
from app.services.logging_config import get_logger
import redis.asyncio as redis
import time
# Load bi·∫øn m√¥i tr∆∞·ªùng v√† l·∫•y danh s√°ch API keys
load_dotenv()
logger = get_logger()
keys = os.environ.get("GEMINI_API_KEY", "").split(",")

from app.lib.redis_client import r
async def init_api_key_zset(keys):
    logger.info("Successed in initializing api keys set")
    # ch·ªâ ch·∫°y m·ªôt l·∫ßn l√∫c kh·ªüi ƒë·ªông
    now = 0.0
    # mapping API key -> score
    mapping = { key: now for key in keys }
    # n·∫øu c·∫ßn ƒë·∫£m b·∫£o ZSET tr·ªëng
    await r.delete("gemini_api_keys")
    # th√™m t·∫•t c·∫£ keys v·ªõi score 0
    await r.zadd("gemini_api_keys", mapping)

# Logger


async def get_lru_api_key():
    while True:
        result = await r.zrange("gemini_api_keys", 0, 0, withscores=True)
        if not result:
            raise RuntimeError("None of api keys are presence in the API keys set")

        key, _old_score = result[0]

        key = key.decode()
        idx = keys.index(key)
        logger.info(f"Key {idx} has been selected")
        now_ts = time.time()
        updated = await r.zadd(
            "gemini_api_keys",
            {key: now_ts},
            xx=True,
            ch=True
        )
        if updated:
            return key

        await asyncio.sleep(0)


async def GeminiAgent(
    model: str,
    contents: Union[types.ContentListUnion, types.ContentListUnionDict],
    config: types.GenerateContentConfigOrDict,
    retry_delay: float = 1.0,
    max_retries: int = 2
):
    delay = retry_delay
    retry_count = 0

    while True:
        # L·∫•y key LRU
        api_key = await get_lru_api_key()
        client = genai.Client(api_key=api_key)

        try:
            response = await client.aio.models.generate_content(
                model=model,
                contents=contents,
                config=config
            )

            if not isinstance(response.text, str) or not response.text.strip():
                raise ValueError("GeminiAgent: response.text is kh√¥ng h·ª£p l·ªá")

            return response

        except errors.APIError as e:
            code = e.code
            msg = e.message or ""
            logger.warning(f"[GeminiAgent] APIError (Code {code}): {msg}")

            if code == 429:
                # Ch·ªâ c·∫ßn log v√† loop ti·∫øp ƒë·ªÉ l·∫•y key kh√°c
                logger.info(f"[GeminiAgent] Key `{api_key}` b·ªã rate-limit, rotate sang key kh√°c.")
                # reset delay cho l·∫ßn d√πng key m·ªõi
                delay = retry_delay
                continue

            if str(code).startswith("5"):
                logger.info(f"[GeminiAgent] L·ªói server {code}, ch·ªù {delay:.1f}s r·ªìi th·ª≠ l·∫°i...")
                await asyncio.sleep(delay)
                delay = min(delay * 1.5, 60)
                continue

            # C√°c l·ªói 4xx kh√°c ‚Äì retry t·ªëi ƒëa
            retry_count += 1
            if retry_count > max_retries:
                logger.error(f"[GeminiAgent] L·ªói client {code} v∆∞·ª£t qu√° {max_retries} l·∫ßn, d·ª´ng.")
                raise
            logger.warning(f"[GeminiAgent] L·ªói client {code}, retry {retry_count}/{max_retries} sau {delay:.1f}s...")
            await asyncio.sleep(delay)
            delay = min(delay * 1.5, 60)

        except Exception as e:
            logger.error(f"[GeminiAgent] L·ªói kh√¥ng x√°c ƒë·ªãnh: {type(e).__name__} ‚Äì {e}")
            raise

if __name__ == "__main__":
    async def main():
        await init_api_key_zset(keys)

        count = 0
        start_time = time.time()

        while True:
            try:
                response = await GeminiAgent(
                    model="gemini-2.0-flash",
                    contents=[
                        types.Content(
                            role="user",
                            parts=[types.Part.from_text(text="xin ch√†o")]
                        )
                    ],
                    config=types.GenerateContentConfig()
                )
                print(f"Ph·∫£n h·ªìi: {response.text.strip()}")
                count += 1

            except Exception as e:
                logger.error(f"[main] L·ªói khi g·ª≠i request: {str(e)}")

            # In th·ªëng k√™ m·ªói ph√∫t
            if time.time() - start_time >= 60:
                print(f"S·ªë l·∫ßn g·ªçi API th√†nh c√¥ng trong 1 ph√∫t: {count}")
                count = 0
                start_time = time.time()

            await asyncio.sleep(1)  # G·ª≠i request m·ªói gi√¢y

    asyncio.run(main())
```

---
File: utils/draw_boxes.py
---
```python
import cv2
import requests
import numpy as np
from PIL import Image
from typing import Tuple, Optional, List
from io import BytesIO 
import concurrent.futures
from functools import lru_cache
import base64
from dotenv import load_dotenv
load_dotenv()
import os
from collections import OrderedDict
# Load model

# --------- CACHE CHO COLORS V√Ä CONSTANTS ----------
COLORS = [(0,255,0), (255,0,0), (0,0,255), (0,255,255), (255,0,255), (255,255,0)]
FONT = cv2.FONT_HERSHEY_SIMPLEX

@lru_cache(maxsize=None)
def get_text_size_cached(text: str, font_scale: float, thickness: int):
    return cv2.getTextSize(text, FONT, font_scale, thickness)


def calculate_max_font_scale(text: str, max_width: int, max_height: int,
                             font_face=cv2.FONT_HERSHEY_SIMPLEX,
                             initial_scale: float = 1.0,
                             scale_step: float = 0.5,
                             min_scale_step: float = 0.01,
                             padding: int = 2) -> Tuple[float, int]:
    def fits(s: float) -> bool:
        th = max(1, int(s * 2))
        (w_text, h_text), baseline = cv2.getTextSize(text, font_face, s, th)
        total_h = h_text + baseline
        return (w_text + 2 * padding) <= max_width and (total_h + 2 * padding) <= max_height

    scale = initial_scale
    if not fits(scale):
        while scale > min_scale_step and not fits(scale):
            scale /= 2
    else:
        while fits(scale):
            scale += scale_step
        scale -= scale_step

    step = scale_step
    while step >= min_scale_step:
        if fits(scale + step):
            scale += step
        else:
            step /= 2

    thickness = max(1, int(scale * 2))
    return scale, thickness


def draw_boxes(
    img: np.ndarray,
    scores: np.ndarray,
    labels: np.ndarray,
    boxes: np.ndarray,
    class_names: Optional[List[str]] = None,
    base_box_thickness: int = 2,
    skip_class_ids: Optional[List[int]] = None,
    skip_class_names: Optional[List[str]] = None,
    draw_labels: bool = True,
    sort_by_coordinate: bool = True,
    row_threshold: int = 10
):
    if boxes is None or len(boxes) == 0:
        return img.copy(), {}

    h_img, w_img = img.shape[:2]
    box_thickness = max(1, int(base_box_thickness * min(w_img, h_img) / 1000))
    out = img.copy()
    skip_ids = set(skip_class_ids or [])
    skip_names = set(skip_class_names or [])

    bboxes = boxes.astype(np.int32)
    valid_indices = []
    for i, bbox in enumerate(bboxes):
        cls = int(labels[i])
        if cls in skip_ids:
            continue
        name = (class_names[cls] if class_names and cls < len(class_names) else str(cls))
        if name in skip_names:
            continue
        x1, y1, x2, y2 = bbox
        if (x2 - x1) < 10 or (y2 - y1) < 10:
            continue
        valid_indices.append(i)

    # Sort theo y-then-x
    if sort_by_coordinate and valid_indices:
        entries = [(i, *bboxes[i][:4]) for i in valid_indices]
        entries.sort(key=lambda e: e[2])  # sort theo y1
        rows = []
        for i, x1, y1, x2, y2 in entries:
            placed = False
            for row in rows:
                y_min, y_max, lst = row
                if y1 <= y_max + row_threshold and y2 >= y_min - row_threshold:
                    row[0] = min(y_min, y1);
                    row[1] = max(y_max, y2)
                    lst.append((i, x1))
                    placed = True
                    break
            if not placed:
                rows.append([y1, y2, [(i, x1)]])
        rows.sort(key=lambda r: r[0])
        sorted_indices = []
        for _, _, group in rows:
            group.sort(key=lambda e: e[1])
            sorted_indices += [i for i, _ in group]
    else:
        sorted_indices = valid_indices

    # T·∫°o dict cho ROI
    cropped_objects_np = OrderedDict()

    for order_idx, i in enumerate(sorted_indices):
        x1, y1, x2, y2 = bboxes[i]
        cls = int(labels[i])
        name = (class_names[cls] if class_names and cls < len(class_names) else str(cls))
        prefix = name[:2].upper() if len(name) >= 2 else name.upper()
        key = f"{prefix}{order_idx + 1}"

        # V·∫Ω khung
        color = COLORS[cls % len(COLORS)]
        cv2.rectangle(out, (x1, y1), (x2, y2), color, box_thickness)

        if draw_labels:
            font_scale, font_thickness = calculate_max_font_scale(
                key, x2-x1, y2-y1, font_face=FONT
            )
            (w_text, h_text), baseline = get_text_size_cached(key, font_scale, font_thickness)
            text_x = np.clip(x1 + (x2-x1 - w_text)//2, x1+2, x2 - w_text - 2)
            text_y = np.clip(y1 + h_text + 2, y1 + h_text + 2, y2 - baseline - 2)

            # ========== Layer 1: N·ªÄN ==========
            background_overlay = out.copy()
            cv2.rectangle(background_overlay, (x1, y1), (x2, y2), (0, 255, 0), -1)
            alpha_background = 0.4  # n·ªÅn m·ªù nh·∫π
            cv2.addWeighted(background_overlay, alpha_background, out, 1 - alpha_background, 0, out)

            # ========== Layer 2: CH·ªÆ ==========
            text_overlay = out.copy()
            cv2.putText(text_overlay, key, (text_x, text_y), FONT, font_scale, (0, 0, 255), font_thickness)
            alpha_text = 0.75  # ch·ªØ g·∫ßn nh∆∞ r√µ r√†ng
            cv2.addWeighted(text_overlay, alpha_text, out, 1 - alpha_text, 0, out)



        roi = img[y1:y2, x1:x2]
        if roi.size > 0:
            cropped_objects_np[key] = roi.copy()

    return out, cropped_objects_np
```

---
File: utils/remove_padding.py
---
```python
import cv2
import numpy as np

def remove_white_padding(image_np: np.ndarray) -> np.ndarray:
    """
    Lo·∫°i b·ªè padding tr·∫Øng xung quanh n·ªôi dung ch√≠nh c·ªßa ·∫£nh.
    Nh·∫≠n v√† tr·∫£ v·ªÅ ·∫£nh d∆∞·ªõi d·∫°ng NumPy array (BGR).
    
    Args:
        image_np (np.ndarray): ·∫¢nh ƒë·∫ßu v√†o d∆∞·ªõi d·∫°ng NumPy (BGR).
        
    Returns:
        np.ndarray: ·∫¢nh ƒë√£ ƒë∆∞·ª£c crop b·ªè padding tr·∫Øng.
    """
    if image_np is None or image_np.size == 0:
        return image_np

    # 1. Convert to grayscale
    gray = cv2.cvtColor(image_np, cv2.COLOR_BGR2GRAY)

    # 2. Invert the image: white ‚Üí 0, text/dark ‚Üí 255
    _, thresh = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY_INV)

    # 3. Find contours
    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if not contours:
        return image_np  # No content found

    # 4. Find bounding box around all contours
    all_contours = np.vstack(contours)
    x, y, w, h = cv2.boundingRect(all_contours)

    # 5. Crop and return
    cropped = image_np[y:y+h, x:x+w]
    return cropped
```

---
File: utils/upload_r2.py
---
```python
# upload_r2.py

import os
import base64
import asyncio
import aioboto3
from urllib.parse import urlparse
from typing import List
from dotenv import load_dotenv
from PIL import Image
from io import BytesIO
# Load environment variables from .env
load_dotenv()

# Required ENV variables
ENDPOINT_URL_R2 = os.getenv("ENDPOINT_URL_R2")
AWS_ACCESS_KEY_ID_R2 = os.getenv("AWS_ACCESS_KEY_ID_R2")
AWS_SECRET_ACCESS_KEY_R2 = os.getenv("AWS_SECRET_ACCESS_KEY_R2")
PUBLIC_URL_R2 = os.getenv("PUBLIC_URL_R2")

if not all([ENDPOINT_URL_R2, AWS_ACCESS_KEY_ID_R2, AWS_SECRET_ACCESS_KEY_R2, PUBLIC_URL_R2]):
    raise RuntimeError("Missing required environment variables for R2")

# Parse bucket and endpoint
parsed = urlparse(ENDPOINT_URL_R2)
bucket_name = parsed.path.lstrip("/")  # remove leading slash
endpoint_base = f"{parsed.scheme}://{parsed.netloc}"

# === FNV-1a hash (simple 32-bit) ===
def fnv1a_hash(buffer: bytes) -> str:
    hash_ = 0x811c9dc5
    for byte in buffer:
        hash_ ^= byte
        hash_ = (hash_ * 0x01000193) % (1 << 32)
    return format(hash_, "08x")

# === Upload single image (base64 JPEG) ===
async def upload_img_to_r2(buffer: bytes) -> str:
    with Image.open(BytesIO(buffer)) as img:
        output = BytesIO()
        img.convert("RGB").save(output, format="JPEG", quality=85, optimize=True)
        jpeg_bytes = output.getvalue()

    key = fnv1a_hash(jpeg_bytes) + ".jpg"

    session = aioboto3.Session()
    async with session.client(
        "s3",
        endpoint_url=endpoint_base,
        aws_access_key_id=AWS_ACCESS_KEY_ID_R2,
        aws_secret_access_key=AWS_SECRET_ACCESS_KEY_R2,
        region_name="auto",
    ) as s3:
        await s3.put_object(
            Bucket=bucket_name,
            Key=key,
            Body=jpeg_bytes,
            ContentType="image/jpeg",
            ACL="public-read"
        )

    return f"{PUBLIC_URL_R2}/{key}"
# === Upload multiple images with concurrency limit ===
async def upload_multiple_images(buffers: List[bytes], concurrency_limit: int = 5) -> List[str]:
    sem = asyncio.Semaphore(concurrency_limit)

    async def limited_upload(buffer: bytes):
        async with sem:
            return await upload_img_to_r2(buffer)

    tasks = [limited_upload(buf) for buf in buffers]
    return await asyncio.gather(*tasks)
```

---
File: worker/__init__.py
---
```python

```

---
File: worker/__pycache__/__init__.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode bytes in position 9-10: invalid continuation byte

---
File: worker/__pycache__/core.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xc0 in position 8: invalid start byte

---
File: worker/__pycache__/extractor_worker.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xfa in position 8: invalid start byte

---
File: worker/__pycache__/instance.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x8b in position 10: invalid start byte

---
File: worker/__pycache__/process_img.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xbf in position 8: invalid start byte

---
File: worker/__pycache__/process_pdf.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xfb in position 8: invalid start byte

---
File: worker/instance.py
---
```python
from app.lib.worker_core import SimpleRedisWorker

worker = SimpleRedisWorker()
```

---
File: worker/tasks/__init__.py
---
```python

```

---
File: worker/tasks/__pycache__/__init__.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode bytes in position 9-10: invalid continuation byte

---
File: worker/tasks/__pycache__/document_parser.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x90 in position 10: invalid start byte

---
File: worker/tasks/__pycache__/extractor.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0xa8 in position 8: invalid start byte

---
File: worker/tasks/__pycache__/process_img.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x90 in position 10: invalid start byte

---
File: worker/tasks/__pycache__/process_pdf.cpython-310.pyc
---
Error reading file: 'utf-8' codec can't decode byte 0x90 in position 10: invalid start byte

---
File: worker/tasks/document_parser.py
---
```python
import asyncio
import logging

from google.genai import types
import re
from app.worker.instance import worker
from app.utils.agent import GeminiAgent
from app.prompt import (
    get_initial_slurp_prompt,
    get_slurp_continuation_prompt,
    get_initial_slurp_examples,
    get_slurp_continuation_examples
)
import json  
from app.postprocessing.replaceimgfig import replace_img_to_fig, replace_fig2img_immutable
from app.db.client import get_session
from app.db.models import TaskType
from app.services.create_task import create_task, update_task_result
from app.postprocessing.slurp2json import slurp_to_json
from app.services.logging_config import get_logger

logger = get_logger()

PARSER_MODEL = "gemini-2.0-flash-001"
MAX_CONTINUATION_ATTEMPTS = 6



def clean_output(text: str) -> str:
    # Lo·∫°i b·ªè t·∫•t c·∫£ marker @slurp_...@
    return re.sub(r"@slurp(_resume)?(_incomplete)?_(start|end)@", "", text).strip()

async def llmAsParser(text: str):
    text, figures = replace_img_to_fig(text)

    initial_contents = get_initial_slurp_examples() + [
        types.Content(
            role="user",
            parts=[types.Part.from_text(text=f"@mml_start@{text}@mml_end@")]
        )
    ]
    initial_instruction = get_initial_slurp_prompt()
    initial_config = types.GenerateContentConfig(
        system_instruction=[types.Part.from_text(text=initial_instruction)],
        temperature=0
    )

    initial_response = await GeminiAgent(
        model=PARSER_MODEL,
        contents=initial_contents,
        config=initial_config
    )
    slurp_content = initial_response.text

    if "@slurp_start@" in slurp_content and "@slurp_end@" in slurp_content:
        return clean_output(slurp_content), figures

    if "@slurp_start@" not in slurp_content:
        raise ValueError("Parser did not return start marker.")

    slurp_content = clean_output(slurp_content)
    cont_instruction = get_slurp_continuation_prompt()
    continuation_config = types.GenerateContentConfig(
        system_instruction=[types.Part.from_text(text=cont_instruction)],
        temperature=0
    )
    continuation_examples = get_slurp_continuation_examples()

    for attempt in range(MAX_CONTINUATION_ATTEMPTS):
        cont_contents = continuation_examples + [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(
                        text=f"@mml_start@{text}@mml_end@\n"
                             f"@slurp_incomplete_start@{slurp_content}@slurp_incomplete_end@"
                    )
                ]
            )
        ]

        cont_response = await GeminiAgent(
            model=PARSER_MODEL,
            contents=cont_contents,
            config=continuation_config,
        )
        cont_output = cont_response.text

        if "@slurp_resume_end@" in cont_output or "@slurp_end@" in cont_output:
            slurp_content += clean_output(cont_output)
            return slurp_content, figures

        slurp_content += clean_output(cont_output)

    logger.info("Parser failed to complete after maximum retries.")
    return slurp_content, figures

@worker.task(name="documentParsing", max_concurrency=20, max_retries=0)
async def parse_document(task_id: str, text: str):
    logger.info(f"[Worker-Parse] Parsing document {task_id}")
    try:
        slurp_content, figures = await llmAsParser(text=text)
        parsed_json = slurp_to_json(slurp_content)
        
        refine_json = replace_fig2img_immutable(parsed_json, figures)
        
        dumped_json = json.dumps(refine_json, ensure_ascii=False)
        
        with get_session() as session:
            update_task_result(session=session, task_id=task_id, result=dumped_json)  
        logger.info(f"[Worker-Parse] Task {task_id} completed successfully.")
    except Exception as e:
        logger.exception(f"[Worker-Parse] Failed to parse document {task_id}: {e}")


if __name__ == "__main__":
    async def main():
        from pathlib import Path
        import json

        # 1. ƒê·ªçc input
        input_file = Path("tests/de15.txt")
        text = input_file.read_text(encoding="utf-8")

        # 2. Ch·∫°y parser
        slurp_content, figures = await llmAsParser(text)

        # 3. Chuy·ªÉn th√†nh JSON v√† thay fig ‚Üí img
        parsed_json = slurp_to_json(slurp_content)
        refine_json = replace_fig2img_immutable(parsed_json, figures)

        # 4. Ghi k·∫øt qu·∫£ cu·ªëi c√πng (c√≥ URL) ra file
        output_path = Path("tests/output.json")
        output_path.write_text(
            json.dumps(refine_json, ensure_ascii=False, indent=2),
            encoding="utf-8"
        )
        print(f"‚úÖ ƒê√£ ghi k·∫øt qu·∫£ c√≥ URL v√†o {output_path}")

    asyncio.run(main())
```

---
File: worker/tasks/extractor.py
---
```python
from typing import List
from app.prompt import get_extraction_figure_prompt, get_extraction_non_figure_prompt
from app.worker.instance import worker
from app.utils.agent import GeminiAgent
from google.genai import types
import asyncio
from app.main import get_file_bytes
from PIL import Image
from io import BytesIO
from app.utils.upload_r2 import upload_multiple_images
from app.postprocessing.raw_response import extract_response, replace_image_tags
import base64
from app.services.create_task import create_log, get_logs_by_task, update_task_result
from app.db.client import get_session
from app.services.counter import decrement_counter, get_counter, delete_counter
from app.db.models import InferenceLog
async def uploadImageFromUrls(ImageUrls) -> List[str]:
    image_bytes = await asyncio.gather(*(get_file_bytes(img) for img in ImageUrls))
    image_urls = await upload_multiple_images(image_bytes, concurrency_limit=10)
    return image_urls

from app.services.logging_config import get_logger

logger = get_logger()


async def convert_to_webp_base64(img_bytes: bytes, quality: int = 80) -> str:
    with Image.open(BytesIO(img_bytes)) as img:
        output = BytesIO()
        img.convert("RGBA").save(output, format="WEBP", quality=quality, method=6)
        webp_bytes = output.getvalue()
        base64_str = base64.b64encode(webp_bytes).decode('utf-8')
        return base64_str

@worker.task(name="parseDocumentImage", max_concurrency=20,max_retries=0)
async def extractDocumentImage(task_id : str, img_url : str, page_order: int, cropped_objects_urls):
    logger.info(f"[Worker-Document] Start processing task {task_id}")

    img_bytes = await get_file_bytes(img_url)
    img_webp = await convert_to_webp_base64(img_bytes)

    object_urls = []
    object_keys = []
    uploadedUrls = []
    for key, url in cropped_objects_urls:
        object_urls.append(url)
        object_keys.append(key)
    
    model = "gemini-2.0-flash-001"
    generate_content_config = types.GenerateContentConfig(
        media_resolution="MEDIA_RESOLUTION_HIGH",
        system_instruction=[
            types.Part.from_text(text=get_extraction_figure_prompt()),
        ],
    )

    user_parts = [
        types.Part.from_bytes(
            mime_type="image/webp",
            data=base64.b64decode(img_webp),
        ),
    ]

    # if object_urls:
    #     user_parts.append(types.Part.from_text(text="""a"""))

    contents = [
        types.Content(
            role="user",
            parts=user_parts,
        ),
    ]


    if(object_urls):

        llmResponse, uploadedUrls = await asyncio.gather(
            GeminiAgent(model=model,contents=contents,config=generate_content_config),
            uploadImageFromUrls(object_urls))
    else:
        generate_content_config = types.GenerateContentConfig(
            media_resolution="MEDIA_RESOLUTION_HIGH",
            system_instruction=[
                types.Part.from_text(text=get_extraction_non_figure_prompt()),
            ],
            
        )
        llmResponse = await GeminiAgent(model=model,contents=contents,config=generate_content_config)
    if not llmResponse.text:
        raise ValueError("D·ªØ li·ªáu kh√¥ng h·ª£p l·ªá!")
    with get_session() as session:
        create_log(imageUrls=img_url,
                objectKeys=object_keys,
                objectUrls=uploadedUrls,
                requestId=task_id,
                num_input_token=llmResponse.usage_metadata.prompt_token_count or 0,
                num_output_token=llmResponse.usage_metadata.candidates_token_count or 0,
                rawOutput=llmResponse.text, page_order=page_order, session=session)
    if(await get_counter(task_id)):
        await decrement_counter(task_id)
    else:
        await delete_counter(task_id)
        combined_output = ''
        
        with get_session() as session:
            logger.info(f"[Worker-Document] Finished All Call Request {task_id}")
            logs: list[InferenceLog] = get_logs_by_task(session=session, task_id=task_id)
            sorted_logs: list[InferenceLog] = sorted(logs, key=lambda log: log.page_order)  # IDE s·∫Ω suy ƒë∆∞·ª£c ki·ªÉu

            for log in sorted_logs:
                doc_block = extract_response(log.rawOutput).document
                if(log.objectUrls):
                    doc_block, status = replace_image_tags(doc_block,  dict(zip(list(log.objectKeys), list(log.objectUrls))))
                if(doc_block):
                    combined_output += doc_block 
                
            update_task_result(session=session, result=combined_output, task_id=task_id)
            logger.info(f"[Worker-Document] Writing Task result {task_id}")
```

---
File: worker/tasks/process_img.py
---
```python
# worker/extractor_worker.py
import asyncio
import os
from app.db.client import get_session
from app.db.models import Task, TaskStatus
import fitz
import numpy as np
import requests
import hashlib
from app.main import save_file, get_file_bytes
from pathlib import Path
import io
import base64
import cv2
from io import BytesIO
from PIL import Image
from app.ml_models.image_classifier import classifier
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from app.utils.remove_padding import remove_white_padding
from app.utils.draw_boxes import draw_boxes
from app.ml_models.rfdetr import rtdetr_model
from app.utils.upload_r2 import upload_multiple_images # <-- IMPORT M·ªöI
from collections import OrderedDict # <-- IMPORT M·ªöI
from app.worker.instance import worker

import io
import cv2
import numpy as np
from typing import Dict, Tuple
from pathlib import Path
from PIL import Image
import asyncio

JPEG_QUALITY = 90
PIL_JPEG_PARAMS = {'format': 'JPEG', 'quality': JPEG_QUALITY}
JPEG_PARAMS = [int(cv2.IMWRITE_JPEG_QUALITY), 90]
from app.services.logging_config import get_logger

logger = get_logger()

def get_annotated_images(image: np.ndarray,  threshold: float = 0.5):
    cropped_image = remove_white_padding(image)
    pil_image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))
    scores, labels, boxes = rtdetr_model.run_inference(pil_image, confidence_threshold=threshold)
    
    # draw_boxes gi·ªù s·∫Ω tr·∫£ v·ªÅ `OrderedDict` c·ªßa c√°c numpy array
    out_np, cropped_objects_np = draw_boxes(
        cropped_image.copy(), scores, labels, boxes,
        class_names=["Image", "Table"],
        skip_class_ids=[1], skip_class_names=["Table"],
        draw_labels=True
    )
    return out_np, cropped_objects_np


BASE_URL = "http://localhost:8000/media/"

async def save_cropped_objects_to_urls(
    cropped_objects_np: Dict[str, np.ndarray],
    image_format: str = "jpg"
) -> Dict[str, str]:


    async def encode_and_save(key: str, img: np.ndarray) -> Tuple[str, str]:
        # Encode np.ndarray -> bytes (JPG)
        is_success, buffer = cv2.imencode(f".{image_format}", img)
        if not is_success:
            raise ValueError(f"Failed to encode image for key: {key}")
        img_bytes = buffer.tobytes()

        # G·ªçi h√†m l∆∞u file
        relative_path: Path = await save_file(img_bytes, image_format)

        # Tr·∫£ l·∫°i URL ƒë·∫ßy ƒë·ªß
        return key, f"{BASE_URL}{relative_path.as_posix()}"

    # Ch·∫°y ƒë·ªìng th·ªùi t·∫•t c·∫£ encode + save
    tasks = [encode_and_save(key, img) for key, img in cropped_objects_np.items()]
    results = await asyncio.gather(*tasks)

    return dict(results)


# H√†m n√†y c·∫ßn l√† async v√¨ n√≥ s·∫Ω g·ªçi upload_multiple_images
async def annotate_img(img_np: np.ndarray):
    pred_idx, logits, confidence = classifier.predict(img_np)

    # Kh·ªüi t·∫°o ƒë·ªÉ tr√°nh l·ªói n·∫øu kh√¥ng ph·∫£i lo·∫°i c·∫ßn x·ª≠ l√Ω
    processed_img_np = img_np.copy()
    cropped_objects_np = {}

    if pred_idx == 1:
        processed_img_np, cropped_objects_np = get_annotated_images(img_np, threshold=0.5)

    # L∆∞u c√°c object ƒë∆∞·ª£c crop
    cropped_objects_urls = await save_cropped_objects_to_urls(cropped_objects_np)

    is_success, buffer = cv2.imencode(".jpg", processed_img_np)
    if not is_success:
        raise ValueError("Failed to encode processed image")

    processed_img_bytes = buffer.tobytes()
    processed_relative_path: Path = await save_file(processed_img_bytes, "jpg")
    processed_img_url = f"{BASE_URL}{processed_relative_path.as_posix()}"

    return processed_img_url, cropped_objects_urls




@worker.task(name="process_img",max_concurrency=1, max_retries=1)
async def process_img(task_id :str,page_idx: int, file_url: str):
    logger.info(f"[Worker-Image] Start processing task {task_id}")
    img_bytes = await get_file_bytes(file_url)
    img = Image.open(BytesIO(img_bytes))
    
    # G·ªçi h√†m annotate_img (gi·ªù ƒë√£ l√† async)
    processed_img_url , cropped_objects_urls = await annotate_img(img_np=np.array(img))

    await worker.enqueue("parseDocumentImage", task_id , processed_img_url, page_idx, list(cropped_objects_urls.items()))

    # T·∫°i ƒë√¢y, cropped_objects_urls ƒë√£ c√≥ c·∫•u tr√∫c Key-URL nh∆∞ mong mu·ªën
    # TODO: C·∫≠p nh·∫≠t task trong DB v·ªõi th√¥ng tin cropped_objects_urls
    # V√≠ d·ª• (c·∫ßn import json n·∫øu l∆∞u v√†o Text field, ho·∫∑c c√≥ th·ªÉ l∆∞u d∆∞·ªõi d·∫°ng JSONB n·∫øu DB h·ªó tr·ª£):
    # import json
    # from app.db.client import get_session
    # from app.db.models import Task, TaskStatus

    # try:
    #     with get_session() as session:
    #         task = session.query(Task).filter(Task.id == ctx['task_id']).first() # Gi·∫£ ƒë·ªãnh task_id ƒë∆∞·ª£c truy·ªÅn qua ctx
    #         if task:
    #             # K·∫øt qu·∫£ c√≥ th·ªÉ bao g·ªìm c·∫£ ·∫£nh g·ªëc ƒë√£ annotate (n·∫øu mu·ªën) v√† c√°c URL c·ªßa cropped_objects
    #             task.result = json.dumps({
    #                 "annotated_image_b64": img_b64, # Ho·∫∑c upload c√°i n√†y l√™n R2 v√† l∆∞u URL
    #                 "cropped_objects": cropped_objects_urls
    #             })
    #             task.status = TaskStatus.done
    #             session.commit()
    #             print(f"[Worker] Task {ctx['task_id']} marked as done with cropped objects URLs.")
    #         else:
    #             print(f"[Worker] Task {ctx['task_id']} not found.")
    # except Exception as e:
    #     print(f"[Worker] Error processing task {ctx['task_id']}: {e}")
    #     with get_session() as session:
    #         task = session.query(Task).filter(Task.id == ctx['task_id']).first()
    #         if task:
    #             task.status = TaskStatus.failed
    #             task.error = str(e)
    #             session.commit()
```

---
File: worker/tasks/process_pdf.py
---
```python
# worker/extractor_worker.py

import fitz
import numpy as np
from app.main import save_file, get_file_bytes
from app.worker.instance import worker
from app.services.counter import set_counter
from app.services.logging_config import get_logger

logger = get_logger()
@worker.task(name="process_pdf",max_concurrency=1, max_retries=1)
async def process_pdf(task_id: str, file_url: str):
    logger.info(f"[Worker-PDF] Start processing task {task_id}")
    pdf_bytes = await get_file_bytes(file_url)
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    await set_counter(task_id, len(doc) - 1)
    for page_idx in range(len(doc)):
        page = doc[page_idx]
        pix = page.get_pixmap(dpi=200, alpha=False)
        img_bytes = pix.tobytes("jpeg")
        filepath = await save_file(file_bytes=img_bytes, file_type='jpg')
        file_url = f"http://localhost:8000/media/{filepath}"
        await worker.enqueue("process_img", task_id, page_idx, file_url)
    # try:
    #     with get_session() as session:
    #         task = session.query(Task).filter(Task.id == task_id).first()
    #         if task:
    #             task.status = TaskStatus.done
    #             task.result = "mock result"
    #             session.commit()
    #             print(f"[Worker] Task {task_id} marked as done")
    #         else:
    #             print(f"[Worker] Task {task_id} not found")
    # except Exception as e:
    #     with get_session() as session:
    #         task = session.query(Task).filter(Task.id == task_id).first()
    #         if task:
    #             task.status = TaskStatus.failed
    #             task.error = str(e)
    #             session.commit()
    #     print(f"[Worker] Error processing task {task_id}: {e}")
```

## From: docker-compose.yml

---
File: docker-compose.yml
---
```
services:
  docscanner:
    image: daominhwysi/docscanner:latest
    ports:
      - "8000:8000"
    depends_on:
      - redis
    environment:
      REDIS_HOST: redis
      REDIS_PORT: 6379

  redis:
    image: redis:latest
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data

volumes:
  redis-data:
```

